# -*- coding: utf-8 -*-
"""Predictive_Modeling_Election_Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KgG3ZGe5ahuEuFCOeIO0sOxxU6K_swV8
"""

from google.colab import files

uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.neighbors import KNeighborsClassifier

from sklearn.naive_bayes import GaussianNB

from sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree

from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier

from sklearn.linear_model import LogisticRegression

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from sklearn.neural_network import MLPClassifier

from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

election_df = pd.read_excel('Election_Data.xlsx', sheet_name = 'Election_Dataset_Two Classes')
election_df.head(15)

election_df.info()

"""Out of 10 columns in election_df, 8 columns are of integer datatype and 2 columns are of object datatype"""

election_df.isnull().sum()

"""election_df doesn't contain missing data"""

election_df[election_df.duplicated()]

"""election_df doesn't contain any duplicate data point(row)"""

# Since 'Unnamed: 0' column is of no significance, hence 'Unnamed: 0' is being removed from election_df

election_df.drop('Unnamed: 0', axis = 1, inplace = True)
election_df.head()

"""**Exploratory Data Analysis**"""

print('Number of parties contesting the election are {} and they are {}\n'.format(election_df['vote'].nunique(), election_df['vote'].unique()))

print('Votes each party secured are as follows {}-\n'.format(election_df['vote'].value_counts()))

print('Votes each party secured in % are as follows {}-\n'.format(election_df['vote'].value_counts(normalize = True)))

plt.figure(figsize = (3, 4))
sns.displot(x = 'vote', data = election_df, kind = 'hist')
plt.title('Histplot showing the distribution of votes secured by the contesting parties.')

"""* The election data involves two parties, Labour and Conservative, with Labour securing approximately 70% of the votes (1063 votes) and Conservative securing 30% (462 votes)."""

print('The voters in election_df are of age-\n{}\n'.format(election_df['age'].unique()))

print('Number of voters of different age groups-\n{}\n'.format(election_df['age'].value_counts()))

print('Percentage of voters of different age groups-\n{}\n'.format(election_df['age'].value_counts(normalize = True)))

plt.figure(figsize = (35, 6))
sns.countplot(x = 'age', data = election_df)
plt.grid()
plt.title('Countplot showing the number of voters of different age.')

plt.figure(figsize = (45, 10))
sns.countplot(x = 'age', data = election_df, hue = 'vote')
plt.grid()
plt.title('Number of voters of all age who voted for the Conservative and Labour.')

"""* Voter ages range from 24 to 93, with varying numbers of voters in each age group.

* The majority of voters, except those aged 90, 92, and 93, had voted for the Labour Party.

* Voters aged 90, 92, & 93 had voted for the Conservative party.
"""

print('Number of male and female voters in election_df are-\n{}\n'.format(election_df['gender'].value_counts()))

print('Percentage of male and female voters in election_df are-\n{}\n'.format(election_df['gender'].value_counts(normalize = True)))

plt.figure(figsize = (4, 3))
sns.countplot(x = 'gender', data = election_df)
plt.title("Number of male and female voters in election_df.")

plt.figure(figsize = (8, 3))
sns.countplot(x = 'gender', data = election_df, hue = 'vote')
plt.title("Number of male and female voters who voted for the Conservative and Labour party.")

"""* The election data includes both male and female voters, with females comprising 53.25% and males 46.75% of the electorate.

* The majority of both male and female voters supported the Labour Party; however, the majority of voters who voted for the Conservative party are female voters.
"""

#Assessment of current national economic condition

print('The assessment of the current national economic condition by the voters are as follows-\n{}\n'.format(election_df['economic.cond.national'].value_counts()))

print('The assessment of the current national economic condition by the voters in percentage are as follows-\n{}\n'.format(election_df['economic.cond.national'].value_counts(normalize = 1)))

plt.figure(figsize = (8, 4))
sns.countplot(x = 'economic.cond.national', data = election_df)
plt.title('Assessment of current national economic contition by the voters')

plt.figure(figsize = (8, 4))
sns.countplot(x = 'economic.cond.national', data = election_df, hue = 'vote')
plt.title('Assessment of current national economic contition by the voters and the party they voted')

"""* The majority of voters with a strong assessment of the national economic condition voted for the Labour Party, while those with a weak assessment favored the Conservative Party."""

#Assessment of current household economic condition

print('The assessment of the current household economic condition by the voters are as follows-\n{}\n'.format(election_df['economic.cond.household'].value_counts()))

print('The assessment of the current household economic condition by the voters in percentage are as follows-\n{}\n'.format(election_df['economic.cond.household'].value_counts(normalize = 1)))

plt.figure(figsize = (8, 4))
sns.countplot(x = 'economic.cond.household', data = election_df)
plt.title('Assessment of current household economic contition by the voters')

plt.figure(figsize = (8, 4))
sns.countplot(x = 'economic.cond.household', data = election_df, hue = 'vote')
plt.title('Assessment of current household economic contition by the voters and the party they voted')

"""* The majority of voters, regardless of their household economic assessment, voted for the Labour Party, suggesting that household economic conditions were not a strong determinant in vote choice."""

#Assessment of the Labour leader

print('The assessment of the Labour leader by the voters are as follows-\n{}\n'.format(election_df['Blair'].value_counts()))

print('The assessment of the Labour leader by the voters in percentage are as follows-\n{}\n'.format(election_df['Blair'].value_counts(normalize = 1)))

plt.figure(figsize = (8, 4))
sns.countplot(x = 'Blair', data = election_df)
plt.title('Assessment of the Labour party leader by the voters')

plt.figure(figsize = (8, 4))
sns.countplot(x = 'Blair', data = election_df, hue = 'vote')
plt.title('Assessment of the Labour party leader by the voters and the party they voted')

"""* A strong assessment of the Labour leader correlated with votes for the Labour party"""

#Assessment of the Conservative leader

print('The assessment of the Conservative leader by the voters are as follows-\n{}\n'.format(election_df['Hague'].value_counts()))

print('The assessment of the Conservative leader by the voters in percentage are as follows-\n{}\n'.format(election_df['Hague'].value_counts(normalize = 1)))

plt.figure(figsize = (8, 4))
sns.countplot(x = 'Hague', data = election_df)
plt.title('Assessment of the Conservative party leader by the voters')

plt.figure(figsize = (8, 4))
sns.countplot(x = 'Hague', data = election_df, hue = 'vote')
plt.title('Assessment of the Conservative party leader by the voters and the party they voted')

"""* A weak assessment correlated with votes for the Conservative party."""

#Voters attitude toward European integration

print('The assessment of the voters attitude toward European integration are as follows-\n{}\n'.format(election_df['Europe'].value_counts()))

print('The assessment of the voters attitude toward European integration in percentage are as follows-\n{}\n'.format(election_df['Europe'].value_counts(normalize = 1)))

plt.figure(figsize = (8, 4))
sns.countplot(x = 'Europe', data = election_df)
plt.title('Assessment of the voters attitude toward European integration')

plt.figure(figsize = (8, 4))
sns.countplot(x = 'Europe', data = election_df, hue = 'vote')
plt.title('Assessment of the voters attitude toward European integration and the party they voted')

"""* Voters who rated European integration from 1 to 8 mostly voted for Labour, while those who rated it from 9 to 11 favored the Conservative party."""

# Knowledge of parties positions on European integration

print('Voters knowledge of parties position on European integration-\n{}\n'.format(election_df['political.knowledge'].value_counts()))

print('Voters knowledge of parties position on European integration in percentage are as follows-\n{}\n'.format(election_df['political.knowledge'].value_counts(normalize = 1)))

plt.figure(figsize = (6, 4))
sns.countplot(x = 'political.knowledge', data = election_df)
plt.title('Voters knowledge of parties position on European integration')

plt.figure(figsize = (6, 4))
sns.countplot(x = 'political.knowledge', data = election_df, hue = 'vote')
plt.title('Voters knowledge of parties position on European integration and the party they voted')

"""* The majority of voters, regardless of their political knowledge, voted for the Labour Party, suggesting that political knowledge is not a strong determinant in vote choice ."""

# Descriptive statistics of election_df

election_df.describe()

plt.figure(figsize = (10, 4))
sns.heatmap(election_df[['age', 'economic.cond.national', 'economic.cond.household', 'Blair', 'Hague', 'Europe', 'political.knowledge']].corr(), annot = True)

for col in election_df.columns:
  if election_df[col].dtype == 'object':
    print('feature: ', col)
    print(pd.Categorical(election_df[col].unique()))
    print(pd.Categorical(election_df[col].unique()).codes)
    election_df[col] = pd.Categorical(election_df[col]).codes
    print('\n')

X = election_df[['age', 'economic.cond.national',	'economic.cond.household', 'Blair',	'Hague', 'Europe', 'political.knowledge',	'gender']]
y = election_df['vote']

X

y.head(10)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 1)

X_train

y_train

X_test

y_test

# KNeighborsClassifier model

knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

# knn_model prediction on train data

y_train_predict = knn_model.predict(X_train)

# knn_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = knn_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)
print('roc_auc_score of knn_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of knn_model on training data.')

# knn_model prediction on test data

y_test_predict = knn_model.predict(X_test)

# knn_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = knn_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of knn_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of knn_model on test data.')

"""**Training Set Performance:**
1. The KNeighborsClassifier model achieves an accuracy of 0.86 on the training set.
2. Metrics for class 0 are moderate (precision=0.79, recall=0.73, F1-score=0.76), while class 1 metrics are robust (precision=0.88, recall=0.91, F1-score=0.90).
3. The ROC AUC score is 0.92, indicating strong discrimination ability.

**Test Set Performance:**
1. The KNeighborsClassifier model achieves an accuracy of 0.78 on the test set.
2. Metrics for class 0 are weak (precision=0.61, recall=0.62, F1-score=0.62), while class 1 metrics are robust (precision=0.85, recall=0.84, F1-score=0.85).
3. The ROC AUC score is 0.83, indicating strong discrimination ability.

**Analysis:** KNeighborsClassifier model exhibits overfitting and shows a drop in performance on the test set, particularly for class 0, indicating potential overfitting and challenges in handling imbalanced data and requires extensive tuning to generalize well.
"""

# Tuning the parameters of the knn_model for better performance.

grid_params = {
                'n_neighbors': [5, 10, 15, 20, 25],
                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
                'leaf_size': [30, 40, 50, 60]
}

grid_search = GridSearchCV(estimator = knn_model, param_grid = grid_params, cv=5, scoring='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

reg_knn_model = grid_search.best_estimator_

# reg_knn_model prediction on train data

y_train_predict = reg_knn_model.predict(X_train)

# reg_knn_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_knn_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)
print('roc_auc_score of reg_knn_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_knn_model on train data.')

# reg_knn_model prediction on test data

y_test_predict = reg_knn_model.predict(X_test)

# reg_knn_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_knn_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)
print('roc_auc_score of reg_knn_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_knn_model on test data.')

"""**Training Set Performance:**
1. The regularized KNeighborsClassifier model achieves an accuracy of 0.83 on the training set.
2. Metrics for class 0 are moderate (precision=0.77, recall=0.64, F1-score=0.70), while class 1 metrics are robust (precision=0.85, recall=0.91, F1-score=0.88).
3. The ROC AUC score is 0.89, indicating strong discrimination ability.

**Test Set Performance:**
1. The regularized KNeighborsClassifier model achieves an accuracy of 0.81 on the test set.
2. Metrics for class 0 are moderate (precision=0.67, recall=0.64, F1-score=0.66), while class 1 metrics are robust (precision=0.86, recall=0.88, F1-score=0.87).
3. The ROC AUC score is 0.87, indicating strong discrimination ability.

**Analysis:** The regularized KNeighborsClassifier model shows consistent performance across both sets, shows a drop in performance on the test set, particularly for class 0, indicating challenges in handling imbalanced data

**GaussianNB model**
"""

# GaussianNB model

gaussian_nb_model = GaussianNB()
gaussian_nb_model.fit(X_train, y_train)

# gaussian_nb_model prediction on train data

y_train_predict = gaussian_nb_model.predict(X_train)

# gaussian_nb_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = gaussian_nb_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)
print('roc_auc_score of gaussian_nb_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of gaussian_nb_model on training data.')

# gaussian_nb_model prediction on test data

y_test_predict = gaussian_nb_model.predict(X_test)

# gaussian_nb_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = gaussian_nb_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)
print('roc_auc_score of gaussian_nb_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of gaussian_nb_model on test data.')

"""**Training Set Performance:**
1. The GaussianNB model achieves an accuracy of 0.83 on the training set.
2. Class 0 metrics are moderate (precision=0.74, recall=0.72, F1-score=0.73), while Class 1 metrics are robust (precision=0.88, recall=0.88, F1-score=0.88),
3. The ROC AUC score is 0.87, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.83.
2. Class 0 metrics are moderate (precision=0.68, recall=0.72, F1-score=0.70), while class 1 metrics are robust (precision=0.89, recall=0.87, F1-score=0.88).
3. The ROC AUC score is 0.88, demonstrating strong predictive power.

**Analysis:**: The GaussianNB model maintains consistent performance across training and test sets, suggesting good generalization capabilities. The model shows drop in performance on the test set particullarly class 0 indicating challengesin handling imbalanced data.

**Decision Tree Classifier**
"""

# Decision Tree Classifier

dtcl = DecisionTreeClassifier(criterion = 'gini', random_state = 1)
dtcl.fit(X_train, y_train)

from google.colab import drive

drive.mount('/content/drive')
file = open('/content/drive/My Drive/Colab Notebooks/vote_tree.dot', 'w')
train_char_label = ['1', '0']
dot_data = export_graphviz(dtcl, out_file = file, feature_names = list(X_train), class_names = list(train_char_label))
file.close()

# dtcl prediction on train data

y_train_predict = dtcl.predict(X_train)

# dtcl performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = dtcl.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)
print('roc_auc_score of dtcl on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of dtcl on training data.')

# dtcl prediction on test data

y_test_predict = dtcl.predict(X_test)

# dtcl performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = dtcl.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)
print('roc_auc_score of dtcl on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of dtcl on test data.')

"""**Training Set Performance:**
1. The DecisionTreeClassifier model achieves an accuracy of 1.00 on the training set.
2. Class 0 and Class1 metrics are exceptional (precision=1.00, recall=1.00, F1-score=1.00)
3. The ROC AUC score is 1.00, demonstrating exceptional predictive power.

**Test Set Performance:**
1. On the test set, the accuracy drops significantly at 0.75 .
2. Class 0 metrics are poor (precision=0.56, recall=0.59, F1-score=0.57), while class 1 metrics are robust (precision=0.83, recall=0.81, F1-score=0.82).
3. The ROC AUC score is 0.70, demonstrating moderate predictive power.

**Analysis:** The DecisionTreeClassifier model exhibits stark contrast across training and test sets suggests overfitting, where the model learns the training data too well, including noise, and fails to generalize to new data particularly for class 0.
"""

# Tuning the parameters of the dtcl for better performance.

params = {
                'criterion': ['gini', 'entropy', 'log_loss'],
                'max_depth': [4, 5, 6, 7, 8, 9, 10, 11],
                'min_samples_split': [2, 5, 7, 9, 11, 13, 15, 17, 19],
                'min_samples_leaf':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
                'max_features':[3, 4, 5, 'log2', 'sqrt']
}

grid_search = GridSearchCV(estimator = dtcl, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

reg_dtcl_model = grid_search.best_estimator_

# Best Hyperparameters: {'criterion': 'gini', 'max_depth': 7, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 17}

# reg_dtcl_model prediction on train data

y_train_predict = reg_dtcl_model.predict(X_train)

# reg_dtcl_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_dtcl_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_dtcl_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_dtcl_model on training data.')

# reg_dtcl_model prediction on test data

y_test_predict = reg_dtcl_model.predict(X_test)

# reg_dtcl_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_dtcl_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_dtcl_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_dtcl_model on test data.')

"""**Training Set Performance:**
1. The regularized DecisionTreeClassifier model achieves an accuracy of 0.85 on the training set.
2. Class 0 metrics are moderate (precision=0.77, recall=0.76, F1-score=0.76), while Class 1 metrics are robust (precision=0.89, recall=0.90, F1-score=0.89),
3. The ROC AUC score is 0.92, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.78 .
2. Class 0 metrics are moderate (precision=0.60, recall=0.70, F1-score=0.65), while class 1 metrics are robust (precision=0.87, recall=0.81, F1-score=0.84).
3. The ROC AUC score is 0.83, demonstrating strong predictive power.

**Analysis:** The regularized DecisionTreeClassifier maintains consistent performance across training and test sets, suggesting good generalization capabilities. There is a drop in performance on the test set, particularly for class 0, indicating potential challenges in handling imbalanced data.

**Random Forest**
"""

rfcl = RandomForestClassifier(random_state = 1)
rfcl.fit(X_train, y_train)

# rfcl model prediction on train data

y_train_predict = rfcl.predict(X_train)

# rfcl model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = rfcl.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of rfcl model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of rfcl model on training data.')

# rfcl model prediction on test data

y_test_predict = rfcl.predict(X_test)

# rfcl model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = rfcl.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of rfcl model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of rfcl model on test data.')

"""**Training Set Performance:**

1. The RandomForestClassifier model achieves an accuracy of 1.00 on the training set.
2. Class 0 and Class1 metrics are exceptional (precision=1.00, recall=1.00, F1-score=1.00)
3. The ROC AUC score is 0.99, demonstrating exceptional predictive power.

**Test Set Performance:**

1. On the test set, the accuracy drops significantly at 0.82 .
2. Class 0 metrics are weak (precision=0.68, recall=0.69, F1-score=0.69), while class 1 metrics are robust (precision=0.88, recall=0.87, F1-score=0.87).
The ROC AUC score is 0.89, demonstrating strong predictive power.

**Analysis:** The RandomForestClassifier model exhibits stark contrast across training and test sets suggests overfitting, where the model learns the training data too well, including noise, and fails to generalize to new data particularly for class 0.
"""

# Tuning the parameters of the rfcl for a regularized random forest classifier model for better performance.

'''
params = {
                'n_estimators' : [100, 125],
                'criterion': ['gini', 'entropy', 'log_loss'],
                'max_depth': [8, 10, 12],
                'min_samples_split': [9, 11, 15, 19],
                'min_samples_leaf':[7, 9, 11, 13],
                'max_features':[3, 4, 5]
}

grid_search = GridSearchCV(estimator = rfcl, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

reg_rfcl_model = grid_search.best_estimator_
'''

# Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 3, 'min_samples_leaf': 9, 'min_samples_split': 11, 'n_estimators': 100}

# Regularized random forest model

reg_rfcl_model1 = RandomForestClassifier(criterion = 'entropy', max_depth = 10, max_features = 3, min_samples_leaf = 9, min_samples_split = 11, n_estimators = 100, random_state = 1)
reg_rfcl_model1.fit(X_train, y_train)

# reg_rfcl_model1 prediction on train data

y_train_predict = reg_rfcl_model1.predict(X_train)

# reg_rfcl_model1 performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_rfcl_model1.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_rfcl_model1 on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_rfcl_model1 on training data.')

# reg_rfcl_model1 prediction on test data

y_test_predict = reg_rfcl_model1.predict(X_test)

# reg_rfcl_model1 performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_rfcl_model1.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_rfcl_model1 on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_rfcl_model1 on test data.')

"""**Training Set Performance:**
1. The regularized RandomForestClassifier model achieves an accuracy of 0.86 on the training set.
2. Class 0 metrics are strong (precision=0.81, recall=0.71, F1-score=0.76), while Class 1 metrics are robust (precision=0.88, recall=0.93, F1-score=0.90),
3. The ROC AUC score is 0.93, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.84 .
2. Class 0 metrics are moderate (precision=0.72, recall=0.70, F1-score=0.71), while class 1 metrics are robust (precision=0.88, recall=0.89, F1-score=0.89).
3. The ROC AUC score is 0.90, demonstrating strong predictive power.

**Analysis:** The regularized RandomForestClassifier maintains consistent performance across training and test sets, suggesting good generalization capabilities. There is a drop in performance on the test set, particularly for class 0, indicating challenges in handling imbalanced data.

**Bagging**
"""

params = {
                'n_estimators' : [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]
}

bgcl = BaggingClassifier(estimator = reg_rfcl_model1, random_state = 1)

grid_search = GridSearchCV(estimator = bgcl, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

bagging_model = grid_search.best_estimator_

# Best Hyperparameters: {'n_estimators': 25}

bagging_model.fit(X_train, y_train)

# bagging_model prediction on train data

y_train_predict = bagging_model.predict(X_train)

# bagging_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = bagging_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of bagging_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of bagging_model on training data.')

# bagging_model prediction on test data

y_test_predict = bagging_model.predict(X_test)

# bagging_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = bagging_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of bagging_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of bagging_model on test data.')

"""**Training Set Performance:**
1. The BaggingClassifier model achieves an accuracy of 0.85 on the training set.
2. Class 0 metrics are moderate (precision=0.81, recall=0.70, F1-score=0.75), while Class 1 metrics are robust (precision=0.87, recall=0.93, F1-score=0.90),
3. The ROC AUC score is 0.92, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.83 .
2. Class 0 metrics are weak (precision=0.71, recall=0.66, F1-score=0.69), while class 1 metrics are robust (precision=0.87, recall=0.89, F1-score=0.88).
3. The ROC AUC score is 0.90, demonstrating strong predictive power.

**Analysis:** The BaggingClassifier maintains consistent performance across training and test sets, suggesting good generalization capabilities. There is a drop in performance on the test set, particularly for class 0, indicating challenges in handling imbalanced data.

**AdaBoostClassifier model**
"""

params = {
                'n_estimators' : [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]
}

adaboost_clf = AdaBoostClassifier(random_state = 1)

grid_search = GridSearchCV(estimator = adaboost_clf, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

adaboost_model = grid_search.best_estimator_

# Best Hyperparameters: {'n_estimators': 15}

# adaboost_model prediction on train data

y_train_predict = adaboost_model.predict(X_train)

# adaboost_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = adaboost_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of adaboost_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of adaboost_model on training data.')

# adaboost_clf model prediction on test data

y_test_predict = adaboost_model.predict(X_test)

# adaboost_clf model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = adaboost_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of adaboost_clf model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of adaboost_clf model on test data.')

"""**Training Set Performance:**
1. The AdaBoostClassifier model achieves an accuracy of 0.83 on the training set.
2. Class 0 metrics are moderate (precision=0.76, recall=0.67, F1-score=0.71), while Class 1 metrics are robust (precision=0.86, recall=0.91, F1-score=0.88),
3. The ROC AUC score is 0.88, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.82 .
2. Class 0 metrics are weak (precision=0.69, recall=0.63, F1-score=0.66), while class 1 metrics are robust (precision=0.86, recall=0.89, F1-score=0.87).
3. The ROC AUC score is 0.88, demonstrating strong predictive power.

**Analysis:** The AdaBoostClassifier maintains consistent performance across training and test sets, suggesting good generalization capabilities. There is a drop in performance on the test set, particularly for class 0, indicating challenges in handling imbalanced data.

**GradientBoosting Classifier**
"""

params = {
                'n_estimators' : [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]
}

gradient_boost_clf = GradientBoostingClassifier(random_state = 1)

grid_search = GridSearchCV(estimator = gradient_boost_clf, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

grad_boost_model = grid_search.best_estimator_

# Best Hyperparameters: {'n_estimators': 60}

# grad_boost_model prediction on train data

y_train_predict = grad_boost_model.predict(X_train)

# grad_boost_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = grad_boost_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of grad_boost_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of grad_boost_model on training data.')

# grad_boost_model prediction on test data

y_test_predict = grad_boost_model.predict(X_test)

# grad_boost_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = grad_boost_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of grad_boost_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of grad_boost_model on test data.')

"""**Training Set Performance:**
1. The GradientBoostingClassifier model achieves an accuracy of 0.88 on the training set.
2. Class 0 metrics are moderate (precision=0.83, recall=0.76, F1-score=0.79), while Class 1 metrics are robust (precision=0.90, recall=0.93, F1-score=0.91),
3. The ROC AUC score is 0.94, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.83.
2. Class 0 metrics are moderate (precision=0.70, recall=0.74, F1-score=0.72), while class 1 metrics are robust (precision=0.89, recall=0.87, F1-score=0.88).
3. The ROC AUC score is 0.90, demonstrating strong predictive power.

**Analysis:** The GradientBoostingClassifier maintains consistent performance across training and test sets, suggesting good generalization capabilities.There is a slight drop in performance on the test set, particularly for class 0, indicating challenges in handling imbalanced data.

**Logistic Regression & Linear Discrminant Analysis**
"""

X = election_df[['age',	'economic.cond.national',	'economic.cond.household', 'Blair',	'Hague', 'Europe', 'political.knowledge', 'gender']]
y = election_df['vote']

X

y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 1)

X_train

y_train

X_test

y_test

ss = StandardScaler()

X_train = ss.fit_transform(X_train)

X_train

X_test = ss.transform(X_test)

X_test

# logistic regression

logistic_model = LogisticRegression(random_state = 1)
logistic_model.fit(X_train, y_train)

# logistic_model prediction on train data

y_train_predict = logistic_model.predict(X_train)

# logistic_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = logistic_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of logistic_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of logistic_model on training data.')

# logistic_model prediction on test data

y_test_predict = logistic_model.predict(X_test)

# logistic_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = logistic_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of logistic_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of logistic_model on test data.')

for i, col in enumerate(X.columns):
  print('coefficient of {} : {}'.format(col, logistic_model.coef_[0][i]))

"""In a Logistic Regression model, the coefficients associated with each feature indicate the feature's impact on the log-odds of the outcome variable (in this case, the 'vote'). A positive coefficient suggests that an increase in the feature's value leads to an increase in the likelihood of voting for 'Labour' (since 'Labour' is encoded as 1), while a negative coefficient suggests the opposite.

*   **age: -0.3228513673330366:** Older voters are slightly less likely to vote for Labour.
*   **economic.cond.national: 0.30071472695893714:** Voters with a more positive view of the national economy are more likely to vote for Labour.
*   **economic.cond.household: 0.14679863897775342:** Voters with a more positive view of their household's economic condition are slightly more likely to vote for Labour.
*   **Blair: 0.6689115660296128:** A positive assessment of Blair (the Labour leader) at the time this data was collected  strongly correlates with a higher likelihood of voting for Labour.
*   **Hague: -1.0163701695164915:** A positive assessment of Hague (the Conservative leader) strongly correlates with a lower likelihood of voting for Labour (or, equivalently, a higher likelihood of voting Conservative). This is the strongest negative predictor.
*   **Europe: -0.7767091890522259:** Voters with a more positive view of European integration are less likely to vote for Labour .
*   **political.knowledge: -0.5071312133962913:** Voters with higher political knowledge are less likely to vote for Labour .
*   **gender: 0.14729551308827907:** Being male (encoded as 1) slightly increases the likelihood of voting for Labour compared to being female (encoded as 0).

**Training Set Performance:**
1. The LogisticRegression model achieves an accuracy of 0.84 on the training set.
2. Class 0 metrics are moderate (precision=0.77, recall=0.69, F1-score=0.73), while Class 1 metrics are robust (precision=0.87, recall=0.91, F1-score=0.89),
3. The ROC AUC score is 0.89, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.82.
2. Class 0 metrics are weak (precision=0.70, recall=0.65, F1-score=0.68), while class 1 metrics are robust (precision=0.87, recall=0.89, F1-score=0.88).
3. The ROC AUC score is 0.88, demonstrating strong predictive power.

**Analysis:** The LogisticRegression maintains consistent performance across training and test sets, suggesting good generalization capabilities. There is a drop in performance on the test set, particularly for class 0, indicating challenges in handling imbalanced data.
"""

# Linear Discriminant Analysis

lda_model = LinearDiscriminantAnalysis()
lda_model.fit(X_train, y_train)

# lda_model prediction on train data

y_train_predict = lda_model.predict(X_train)

# lda_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = lda_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of lda_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of lda_model on training data.')

# lda_model prediction on test data

y_test_predict = lda_model.predict(X_test)

# lda_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = lda_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of lda_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of lda_model on test data.')

for i, col in enumerate(X.columns):
  print('coefficient of {} : {}'.format(col, lda_model.coef_[0][i]))


intercept = lda_model.intercept_
print('\n\nIntercept: ', intercept)

"""In Linear Discriminant Analysis (LDA), the coefficients associated with each feature reflect the feature's contribution to the discriminant function, which is used to separate the classes . These coefficients indicate the magnitude and direction of each feature's effect on distinguishing between the classes.

*   **age: -0.4047553954645554:** Older voters are less likely to vote for Labour.
*   **economic.cond.national: 0.31231620465860266:** Voters with a more positive view of the national economy are more likely to vote for Labour.
*   **economic.cond.household: 0.13918749759953544:** Voters with a more positive view of their household's economic condition are slightly more likely to vote for Labour.
*   **Blair: 0.8267568129082024:** A positive assessment of Blair (the Labour leader) strongly correlates with a higher likelihood of voting for Labour.
*   **Hague: -1.1852921441817834:** A positive assessment of Hague (the Conservative leader) strongly correlates with a lower likelihood of voting for Labour, indicating a higher likelihood of voting Conservative. This is a strong negative predictor.
*   **Europe: -0.8540068585361493:** Voters with a more positive view of European integration are less likely to vote for Labour.
*   **political.knowledge: -0.609673855513653:** Voters with higher political knowledge are less likely to vote for Labour.
*   **gender: 0.1243986678892362:** Being male (encoded as 1) slightly increases the likelihood of voting for Labour.

In **Linear Discriminant Analysis (LDA)**, the **intercept** is a constant term in the decision function that shifts the decision boundary. It represents the value of the discriminant function when all the features are zero.

The **intercept** for the regularized LDA model as **1.42**. The **intercept** helps to calibrate the model's predictions, influencing the overall classification threshold.
"""

lda_model.explained_variance_ratio_

"""**Training Set Performance:**
1. The LinearDiscriminantAnalysis model achieves an accuracy of 0.84 on the training set.
2. Class 0 metrics are moderate (precision=0.76, recall=0.70, F1-score=0.73), while Class 1 metrics are robust (precision=0.87, recall=0.90, F1-score=0.88),
3. The ROC AUC score is 0.89, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.82.
2. Class 0 metrics are weak (precision=0.69, recall=0.66, F1-score=0.67), while class 1 metrics are robust (precision=0.87, recall=0.88, F1-score=0.87).
3. The ROC AUC score is 0.88, demonstrating strong predictive power.

**Analysis:** The LinearDiscriminantAnalysis maintains consistent performance across training and test sets, suggesting good generalization capabilities. There is a drop in performance on the test set, particularly for class 0, indicating challenges in handling imbalanced data.
"""

# Performing model tuning using GridSearchCV

params = {'solver': ['lsqr', 'eigen'],
          'tol': [0.1, 0.001, 0.0001],
          'shrinkage': ['auto']}

grid_search = GridSearchCV(estimator = lda_model, param_grid = params, cv = 7, scoring ='accuracy')

grid_search.fit(X_train, y_train)

print('Best parameters: ', grid_search.best_params_,'\n')

print('Estimator: ', grid_search.best_estimator_)

reg_lda_model = grid_search.best_estimator_

# reg_lda_model prediction on train data

y_train_predict = reg_lda_model.predict(X_train)

# reg_lda_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_lda_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_lda_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_lda_model on training data.')

# reg_lda_model prediction on test data

y_test_predict = reg_lda_model.predict(X_test)

# reg_lda_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_lda_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_lda_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_lda_model on test data.')

for i, col in enumerate(X.columns):
  print('coefficient of {} : {}'.format(col, reg_lda_model.coef_[0][i]))


intercept = reg_lda_model.intercept_
print('\n\nIntercept: ', intercept)

"""In Linear Discriminant Analysis (LDA), the coefficients associated with each feature reflect the feature's contribution to the discriminant function, which is used to separate the classes . These coefficients indicate the magnitude and direction of each feature's effect on distinguishing between the classes.

Based on the provided text, here's an interpretation of the coefficients in the LDA model:

*   **age: -0.3919403578987458:** Older voters are less likely to vote for Labour .
*   **economic.cond.national: 0.36268331672343207:** Voters with a more positive view of the national economy are more likely to vote for Labour .
*   **economic.cond.household: 0.1592505546956438:** Voters with a more positive view of their household's economic condition are slightly more likely to vote for Labour .
*   **Blair: 0.8516336041309318:** A positive assessment of Blair (the Labour leader) strongly correlates with a higher likelihood of voting for Labour .
*   **Hague: -1.1977281585684707:** A positive assessment of Hague (the Conservative leader) strongly correlates with a lower likelihood of voting for Labour, indicating a higher likelihood of voting Conservative . This is a strong negative predictor.
*   **Europe: -0.886348150082236:** Voters with a more positive view of European integration are less likely to vote for Labour .
*   **political.knowledge: -0.5968946748481506:** Voters with higher political knowledge are less likely to vote for Labour .
*   **gender: 0.10710602373164653:** Being male (encoded as 1) slightly increases the likelihood of voting for Labour .

In **Linear Discriminant Analysis (LDA)**, the **intercept** is a constant term in the decision function that shifts the decision boundary. It represents the value of the discriminant function when all the features are zero.

The **intercept** for the regularized LDA model as **1.42**. The **intercept** helps to calibrate the model's predictions, influencing the overall classification threshold.

**Training Set Performance:**
1. The regularized LinearDiscriminantAnalysis model achieves an accuracy of 0.84 on the training set.
2. Class 0 metrics are moderate (precision=0.76, recall=0.72, F1-score=0.74), while Class 1 metrics are robust (precision=0.88, recall=0.90, F1-score=0.89),
3. The ROC AUC score is 0.89, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.82.
2. Class 0 metrics are weak (precision=0.69, recall=0.68, F1-score=0.69), while class 1 metrics are robust (precision=0.88, recall=0.88, F1-score=0.88).
3. The ROC AUC score is 0.89, demonstrating strong predictive power.

**Analysis:** The regularized LinearDiscriminantAnalysis maintains consistent performance across training and test sets, suggesting good generalization capabilities. There is a drop in performance on the test set, particularly for class 0, indicating challenges in handling imbalanced data.

**Artificial Neural Network**
"""

X = election_df[['age',	'economic.cond.national',	'economic.cond.household', 'Blair',	'Hague', 'Europe', 'political.knowledge',	'gender']]
y = election_df['vote']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 121)

mlp_model = MLPClassifier(random_state = 121)
mlp_model.fit(X_train, y_train)

# mlp_model prediction on train data

y_train_predict = mlp_model.predict(X_train)

# mlp_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = mlp_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of mlp_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of mlp_model on training data.')

# mlp_model prediction on test data

y_test_predict = mlp_model.predict(X_test)

# mlp_model performance on train data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = mlp_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of mlp_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of mlp_model on test data.')

"""**Training Set Performance:**
1. The MLPClassifier model achieves an accuracy of 0.84 on the training set.
2. Class 0 metrics are moderate (precision=0.77, recall=0.67, F1-score=0.72), while Class 1 metrics are robust (precision=0.87, recall=0.92, F1-score=0.89),
3. The ROC AUC score is 0.91, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.81.
2. Class 0 metrics are weak (precision=0.74, recall=0.60, F1-score=0.66), while class 1 metrics are robust (precision=0.83, recall=0.90, F1-score=0.86).
3. The ROC AUC score is 0.89, demonstrating strong predictive power.

**Analysis:** The MLPClassifier model maintains consistent performance across training and test sets, suggesting good generalization capabilities. There is a drop in performance on the test set, particularly for class 0, indicating challenges in handling imbalanced data.
"""

# Tuning the parameters of the mlp_model for better performance.

params = {
    'hidden_layer_sizes': [(100), (150), (100, 50), (150, 75), (100,100,100)],
    'activation': ['logistic', 'relu'],
    'solver': ['sgd', 'adam'],
		'learning_rate_init': [0.1, 0.001, 0.0001],
    'tol': [0.1, 0.01, 0.001],
    'max_iter' : [1000, 4000, 7000, 10000],
		'early_stopping':[True, False]
}

grid_search = GridSearchCV(estimator = mlp_model, param_grid = params, cv = 5, scoring = 'accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

reg_mlp_model = grid_search.best_estimator_


# Best Hyperparameters: {'activation': 'logistic', 'early_stopping': False, 'hidden_layer_sizes': (100, 100, 100), 'learning_rate_init': 0.001, 'max_iter': 1000, 'solver': 'adam', 'tol': 0.001}

# reg_mlp_model prediction on train data

y_train_predict = reg_mlp_model.predict(X_train)

# reg_mlp_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_mlp_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_mlp_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_mlp_model on training data.')

# reg_mlp_model prediction on test data

y_test_predict = reg_mlp_model.predict(X_test)

# reg_mlp_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_mlp_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_mlp_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_mlp_model on test data.')

"""**Training Set Performance:**
1. The regularized MLPClassifier model achieves an accuracy of 0.84 on the training set.
2. Class 0 metrics are moderate (precision=0.74, recall=0.72, F1-score=0.73), while Class 1 metrics are robust (precision=0.88, recall=0.89, F1-score=0.89),
3. The ROC AUC score is 0.90, demonstrating strong predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.82.
2. Class 0 metrics are moderate (precision=0.74, recall=0.66, F1-score=0.70), while class 1 metrics are robust (precision=0.85, recall=0.89, F1-score=0.87).
3. The ROC AUC score is 0.88, demonstrating strong predictive power.

**Analysis:** The regularized MLPClassifier model maintains consistent performance across training and test sets, suggesting good generalization capabilities.

**Best Model:**

---

Based on the provided classification reports and analyses, the **GradientBoostingClassifier (grad_boost_model)** appears to be the best model for this classification task.

Here's the reasoning:

1.  **High Discrimination Ability (ROC AUC):**
* The GradientBoostingClassifier achieves the highest ROC AUC score on the test data (0.91), indicating the best overall ability to distinguish between the two classes.
* This surpasses the Regularized RandomForestClassifier (0.90)  and other models like GaussianNB (0.88), LogisticRegression (0.88), LDA (0.88), MLP (0.89) and Regularized MLP (0.88).

2.  **Strong Performance on Minority Class (Class 0):**
* Many models showed weaker performance specifically for Class 0 on the test set.
* The GradientBoostingClassifier achieves the highest F1-score for Class 0 on the test set (0.72).
* This is slightly better than the Regularized RandomForestClassifier (0.71) and notably better than several others like Regularized KNN (0.66) or LDA (0.67).
* A higher F1-score for the minority class is crucial in imbalanced datasets.

3.  **Good Accuracy and Generalization:**
* While the Regularized RandomForestClassifier has slightly higher test accuracy (0.84 vs. 0.83), the GradientBoostingClassifier maintains good accuracy and shows reasonable generalization, with training accuracy at 0.88 and test accuracy at 0.83.
* This gap is acceptable and indicates it's not severely overfitting.

While the Regularized RandomForestClassifier is a very close second with the highest accuracy, the GradientBoostingClassifier's superior performance in ROC AUC and specifically on the harder-to-predict Class 0 gives it a slight edge as the most robust model for this specific dataset and classification problem.
"""

