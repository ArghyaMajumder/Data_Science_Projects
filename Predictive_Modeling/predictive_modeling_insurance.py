# -*- coding: utf-8 -*-
"""Predictive_Modeling_Insurance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q-LqSaFp_VjTGFCXsbLguvKzyhYo5fIx
"""

from google.colab import files

uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.linear_model import LogisticRegression

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from sklearn.neighbors import KNeighborsClassifier

from sklearn.naive_bayes import GaussianNB

from sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree

from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier

from sklearn.neural_network import MLPClassifier

from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

insurance_df = pd.read_csv("insurance_part_data.csv")
insurance_df

# Retrieving info of the columns of insurance_df

insurance_df.info()

# Check for any missing values in insurance_df

insurance_df.isna().sum()

# Check for presence of duplicate rows in insurance_df

insurance_df[insurance_df.duplicated()]

# Drop duplicate rows

insurance_df.drop_duplicates(inplace = True)

insurance_df[insurance_df['Duration'] <= 0]

insurance_df = insurance_df[insurance_df['Duration'] > 0]
insurance_df

print(f"Claimed status of insurance_df: \n{insurance_df['Claimed'].value_counts()}\n")
print(f"Claimed status of insurance_df: \n{insurance_df['Claimed'].value_counts(normalize = True)}\n")

plt.figure(figsize = (4, 3))
sns.histplot(x = 'Claimed', data = insurance_df)
plt.grid(visible = True)
plt.title("Claimed status in insurance_df")

"""* The dataset reveals a notable class imbalance in insurance claims, with 68% of instances (1944 out of 2858) classified as "No" (not claimed) and 32% (914 instances) as "Yes" (claimed)."""

print(f"Number of unique age in insurance_df: {insurance_df['Age'].nunique()}\n")
print(f"Unique age in insurance_df: {insurance_df['Age'].unique()}\n")
print(f"Count of each age in insurance_df:{insurance_df['Age'].value_counts()}\n")

plt.figure(figsize=(20, 6))
sns.histplot(x = 'Age', data = insurance_df)
plt.grid(visible = True)
plt.title("Histplot for the age of insured in insurance_df")

"""* The age of insured individuals spans a wide range (from 8 to 84 years), with a significant concentration around age 36 (880 instances).
This suggests a specific demographic dominates the customer base, which may influence claim patterns.
"""

plt.figure(figsize=(20, 6))
sns.histplot(x = 'Age', data = insurance_df, hue = 'Claimed', multiple = 'dodge')
plt.grid(visible = True)
plt.title("Histplot for the age of insured in insurance_df")

plt.figure(figsize=(10, 4))
sns.scatterplot(x = 'Age', y = 'Claimed', data = insurance_df)
plt.grid(visible = True)
plt.title("Scatterplot illustrating the distribution of the age of insured in insurance_df along with its claimed status")

"""* Histograms and scatterplots reveal that claim likelihood does not strongly correlate with age alone, though certain age groups (e.g., around 36) show higher claim frequencies due to their larger representation in the dataset."""

print(f"Distribution of commission in terms of count in insurance_df:{insurance_df['Commision'].value_counts()}\n")

plt.figure(figsize=(10, 6))
sns.histplot(x = 'Commision', data = insurance_df)
plt.grid(visible = True)
plt.title("Histplot illustrating the distribution the commission received for tour insurance in insurance_df")

"""* Commission values show a high frequency of zero (1239 instances), indicating many policies may be sold without commission."""

plt.figure(figsize=(10, 4))
sns.scatterplot(x = 'Commision', y = 'Claimed', data = insurance_df)
plt.grid(visible = True)
plt.title("Scatterplot illustrating the distribution of commission received for tour insurance along with its claimed status")

plt.figure(figsize=(12, 5))
sns.histplot(x = 'Commision', data = insurance_df, hue = 'Claimed', kde = True, multiple = 'fill')
plt.title("Displot illustrating the distribution of commission received for tour insurance along with its claimed status")

"""* Scatterplots and distribution plots suggest no clear linear relationship between commission amounts and claim status, though lower commission policies often align with non-claimed instances."""

# Sales by agency

total_sales_by_agency = insurance_df.groupby('Agency_Code')['Sales'].sum()
total_sales_by_agency = total_sales_by_agency.reset_index()
total_sales_by_agency

"""* Four agencies dominate total sales, with C2B (86067) leading followed by EPX (50350.02), CWT (31496.55 )& JZI (8632.87)."""

plt.figure(figsize = (10, 6))
sns.histplot(x = 'Sales', data = insurance_df, kde = True)
plt.grid(visible = True)
plt.title('Histplot illustrating the distribution of sales in insurance_df')

"""* Sales distribution highlights varied policy values with potential outliers"""

plt.figure(figsize = (10, 6))
sns.histplot(x = 'Sales', data = insurance_df, hue = 'Claimed', kde = True, multiple = 'fill')
plt.grid(visible = True)
plt.title('Histplot illustrating the distribution of sales for both claimed and unclaimed in insurance_df')

"""* Higher sales values appear to have a slight association with claimed status in histograms with KDE overlays, hinting that more expensive policies might be more likely to result in claims."""

print(f"Number of tour durations in insurance_df: {insurance_df['Duration'].nunique()}\n")
print(f"Duration and number of tours for the said duration in insurance_df:{insurance_df['Duration'].value_counts()}\n")

plt.figure(figsize=(10, 48))
sns.countplot(y = 'Duration', data = insurance_df)
plt.grid(visible = True)
plt.title("Histplot for the no of tours for each duration type in insurance_df")

"""* Tour durations vary widely (255 unique values), with shorter durations like 10 days (78 instances) being more common.
This variability suggests diverse travel plans among customers.
"""

plt.figure(figsize=(10, 48))
sns.countplot(y = 'Duration', data = insurance_df, hue = 'Claimed')
plt.ylabel('Duration')
plt.grid(visible = True)
plt.title("Histplot for the no of tours for each duration type along with insurance claimed status in insurance_df")

print(f"Number of tour firm selling tour insurance in insurance_df: {insurance_df['Agency_Code'].nunique()}\n")
print(f"Number of insurance sold by each tour firm in insurance_df:{insurance_df['Agency_Code'].value_counts()}\n")

plt.figure(figsize=(5, 3))
sns.histplot(x = 'Agency_Code', data = insurance_df)
plt.xlabel('Agency_Code')
plt.grid(visible = True)
plt.title("Histplot for the number of insurance sold by each tour firm in insurance_df")

"""* Four agencies dominate sales, with EPX selling the most (1238 policies), and five product types exist, with the Customized Plan leading (1070 instances) indicating a preference for certain agencies and products."""

plt.figure(figsize=(5, 3))
sns.histplot(x = 'Agency_Code', data = insurance_df, hue = 'Claimed', multiple = 'dodge')
plt.xlabel('Agency_Code')
plt.grid(visible = True)
plt.title("Histplot for the number of insurance sold by each tour firms along with the number of insurance claimed in insurance_df")

"""* Agency C2B show higher claims, but the proportion of claims varies across agencies, indicating agency-specific risk profiles or customer behaviors."""

plt.figure(figsize=(10, 4))
sns.histplot(x = 'Agency_Code', data = insurance_df, hue = 'Type', multiple = 'dodge')
plt.xlabel('Agency_Code')
plt.grid(visible = True)
plt.title("Histplot illustrating the type of tour insurance being sold by each tour firm in insurance_df")

"""* Agencies like C2B & JZI are Airlines whereas agencies like EPX & CWT are Travel Agency."""

plt.figure(figsize=(12, 4))
sns.histplot(x = 'Agency_Code', data = insurance_df, hue = 'Channel', multiple = 'dodge')
plt.xlabel('Agency_Code')
plt.grid(visible = True)
plt.title("Histplot illustrating the number of tour insurance sold by each agency through different channels in insurance_df")

"""* C2B & JZI sold insurance online.
* EPX & CWT sold the vast majority of insurance is sold online followed by offline.
"""

print(f"Type of tour insurance firms in insurance_df: {insurance_df['Type'].nunique()}\n")
print(f"Number of tours conducted by each type of tour insurance firm in insurance_df:{insurance_df['Type'].value_counts()}\n")

plt.figure(figsize=(5, 3))
sns.histplot(x = 'Type', data = insurance_df, )
plt.ylabel('Type')
plt.grid(visible = True)
plt.title("Histplot for the no of tours for each type of tour insurance firms in insurance_df")

"""* Travel Agencies (1709 instances) sell more through online channels compared to Airlines (1149 instances), with specific agencies like EPX heavily favoring online sales. This interaction highlights channel preferences based on agency type."""

plt.figure(figsize=(5, 3))
sns.histplot(x = 'Type', data = insurance_df, hue = 'Claimed', multiple = 'dodge')
plt.ylabel('Type')
plt.grid(visible = True)
plt.title("Histplot for the no of tours for each type of tour insurance firms along with insurance claimed status in insurance_df")

# Distribution channel of tour insurance agencies (Channel) Distibution channel

print(f"Number of channels through which the insurance firm is providing tour insurance: {insurance_df['Channel'].nunique()}\n")
print(f"Number of insurance bought through each channel: \n{insurance_df['Channel'].value_counts()}\n")

plt.figure(figsize=(5, 3))
sns.histplot(x = 'Channel', data = insurance_df)
plt.xlabel('Channel')
plt.grid(visible = True)
plt.title("Histplot illustrating the number of insurance bought through each channel in insurance_df")

plt.figure(figsize=(7, 4))
sns.histplot(x = 'Channel', data = insurance_df, hue = 'Agency_Code', multiple = 'dodge')
plt.ylabel('Channel')
plt.grid(visible = True)
plt.title("Histplot illustrating number of insurance sold by each tour firm through different channels in insurance_df")

plt.figure(figsize=(5, 4))
sns.histplot(x = 'Channel', data = insurance_df, hue = 'Type', multiple = 'dodge')
plt.xlabel('Channel')
plt.grid(visible = True)
plt.title("Histplot illustrating the type of tour insurance firm and number of insurance sold by them through different channels in insurance_df")

plt.figure(figsize=(5, 4))
sns.histplot(x = 'Channel', data = insurance_df, hue = 'Claimed', multiple = 'dodge')
plt.xlabel('Channel')
plt.grid(visible = True)
plt.title("Histplot illustrating the number of insurance bought through each channel with claimed status in insurance_df")

"""* The vast majority of insurance is sold online (2812 instances) compared to offline (46 instances), with online channels showing a slightly higher proportion of claims, possibly due to volume."""

# Name of the tour insurance products (Product)

print(f"Number of products sold by tour insurance firms: {insurance_df['Product Name'].nunique()}\n")
print(f"Sales of each product:\n{insurance_df['Product Name'].value_counts()}\n")

plt.figure(figsize=(12, 4))
sns.histplot(x = 'Product Name', data = insurance_df)
plt.xlabel('Product_Name')
plt.grid(visible = True)
plt.title("Histplot for the number of sales of each tour insurance products in insurance_df")

"""* Among the various products, Customised Plan were most sold followed by Bronze Plan, Cancellation Plan, Silver Plan & Gold Plan


"""

plt.figure(figsize=(12, 4))
sns.histplot(x = 'Product Name', data = insurance_df, hue = 'Agency_Code', multiple = 'dodge')
plt.xlabel('Product_Name')
plt.grid(visible = True)
plt.title("Histplot for the number of sales of each tour insurance products by each tour agency in insurance_df")

"""* Four agencies dominate sales, with EPX selling the most (1238 policies), and five product types exist, with the Customized Plan leading (1070 instances).

* Sales of Customized Plan is dominated by C2B, EPX & CWT.  

* Sales of Cancellation Plan is dominated by EPX.

* Sales of Bronze Plan is dominated by C2B, CWT & JZI.

* Sales of Silver Plan is dominated by C2B & CWT.

* Sales of Gold Plan is dominated by C2B & CWT.
"""

plt.figure(figsize=(12, 3))
sns.histplot(x = 'Product Name', data = insurance_df, hue = 'Channel', multiple = 'dodge')
plt.xlabel('Product_Name')
plt.grid(visible = True)
plt.title("Histplot for the number of sales of each tour insurance products through each distribution channel in insurance_df")

"""* The vast majority of insurance is sold online (2812 instances) compared to offline (46 instances)."""

plt.figure(figsize=(12, 3))
sns.histplot(x = 'Product Name', data = insurance_df, hue = 'Type', multiple = 'dodge')
plt.xlabel('Product_Name')
plt.grid(visible = True)
plt.title("Histplot for the number of sales of each tour insurance products by each insurance firm in insurance_df")

plt.figure(figsize=(12, 3))
sns.histplot(x = 'Product Name', data = insurance_df, hue = 'Claimed', multiple = 'dodge')
plt.xlabel('Product_Name')
plt.grid(visible = True)
plt.title("Histplot illustrating the claimed status for the each tour insurance products sold in insurance_df")

"""* Most of the claims came from the Silver Plan followed by Bronze Plan, Customized Plan Gold Plan & Cancellation Plan."""

# Destination of the tour (Destination)

print(f"Number of destinations in insurance firms: {insurance_df['Destination'].nunique()}\n")
print(f"Number of tours to each dstinations: \n{insurance_df['Destination'].value_counts()}\n")

plt.figure(figsize=(6, 4))
sns.histplot(x = 'Destination', data = insurance_df)
plt.xlabel('Destination')
plt.grid(visible = True)
plt.title("Histplot for the number of tours to each destination in insurance_df")

"""* Most of the tour insurance buyers are headed to Asia followed by Americas & Europe"""

plt.figure(figsize=(6, 4))
sns.histplot(x = 'Destination', data = insurance_df, hue = 'Agency_Code', multiple = 'dodge')
plt.xlabel('Destination')
plt.grid(visible = True)
plt.title("Histplot for the number of tours to each destination by each agency in insurance_df")

"""* People travelling to Asia bought their insurance from C2B, EPX, CWT & JZI.
* People travelling to Americas bought their insurance from EPX, CWT & JZI.
* People travelling to Eorope bought their insurance from EPX, CWT & JZI.
"""

plt.figure(figsize=(6, 4))
sns.histplot(x = 'Destination', data = insurance_df, hue = 'Type', multiple = 'dodge')
plt.xlabel('Destination')
plt.grid(visible = True)
plt.title("Histplot for the number of tours to each destination by each agency in insurance_df")

"""* Most people travelling to Asia, Americas & Europe bought their insurance through Travel Agency followed by Airlines"""

insurance_df.info()

insurance_df.describe()

plt.figure(figsize=(5, 5))
sns.boxplot(insurance_df[['Age', 'Commision', 'Duration', 'Sales']], orient = 'h')
plt.title("box plot for continuous variables")

"""* Age, Commision, Duration & Sales contains outliers"""

sns.pairplot(insurance_df, hue = 'Claimed')
plt.title("Pairplot with all continuous features in insurance_df")

plt.figure(figsize =(12, 6))
sns.heatmap(insurance_df[['Age', 'Commision', 'Duration', 'Sales']].corr(), annot = True)
plt.title("Heatmap illustrating the correlation among the continuous features in insurance_df")

"""* A heatmap of continuous variables (Age, Commission, Duration, Sales) shows a moderate positive correlation between Sales and Commission, suggesting that higher policy sales often yield higher commissions.
* Correlations with Age and Duration are weak, indicating limited linear relationships among these variables.

"""

# Converting the object types to categorical types

for feature in insurance_df.columns:
  if insurance_df[feature].dtype == 'object':
    print('feature: ', feature)
    print(pd.Categorical(insurance_df[feature].unique()))
    print(pd.Categorical(insurance_df[feature].unique()).codes)
    insurance_df.loc[:, feature] = pd.Categorical(insurance_df[feature]).codes
    print('\n\n')

insurance_df

# using dictionary to convert specific columns
convert_dict = {'Agency_Code': int,
                'Type': int,
                'Claimed': int,
                'Channel': int,
                'Product Name': int,
                'Destination': int}

insurance_df = insurance_df.astype(convert_dict)
insurance_df.info()

insurance_df.head(10)

# Separating the dependent and independent variables

X = insurance_df.drop('Claimed', axis = 1)
y = insurance_df['Claimed']

# Dividing the dependent and independent variables into train and test set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 121)

# Decision Tree Classifier

dtcl = DecisionTreeClassifier(random_state = 121)
dtcl.fit(X_train, y_train)

from google.colab import drive

drive.mount('/content/drive')
file = open('/content/drive/My Drive/Colab Notebooks/insurance_tree.dot', 'w')
train_char_label = ['1', '0']
dot_data = export_graphviz(dtcl, out_file = file, feature_names = list(X_train), class_names = list(train_char_label))
file.close()

# dtcl prediction on train data

y_train_predict = dtcl.predict(X_train)

# dtcl performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = dtcl.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of dtcl on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of dtcl on training data.')

# dtcl prediction on test data

y_test_predict = dtcl.predict(X_test)

# dtcl performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = dtcl.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of dtcl on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of dtcl on test data.')

"""**Training Set Performance:**
The Decision Tree Classifier (dtcl) exhibits exceptional performance on the training data, achieving near-perfect metrics with an accuracy of 1.00, precision, recall, and F1-score close to 1.00 for both classes (0 and 1).
The ROC AUC score is an impressive 0.99995, indicating excellent discrimination capability on the training set.

**Test Set Performance:**
On the test set, the performance drops significantly, with an accuracy of 0.67. Precision, recall, and F1-score for class 0 are around 0.76-0.77, but for class 1, they hover around 0.49-0.51, showing a struggle to predict the minority class effectively.
The ROC AUC score decreases to 0.629, reflecting poorer generalization to unseen data.

**Analysis:** The stark contrast between training and test performance suggests overfitting, where the model learns the training data too well, including noise, and fails to generalize to new data.
"""

# Tuning the parameters of the dtcl for better performance.

reg_dtcl_model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, max_features = 6, min_samples_leaf = 6, min_samples_split= 27, random_state = 121)
reg_dtcl_model.fit(X_train, y_train)

# reg_dtcl_model prediction on train data

y_train_predict = reg_dtcl_model.predict(X_train)

# reg_dtcl_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_dtcl_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_dtcl_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_dtcl_model on training data.')

# reg_dtcl_model prediction on test data

y_test_predict = reg_dtcl_model.predict(X_test)

# reg_dtcl_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_dtcl_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_dtcl_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_dtcl_model on test data.')

"""**Training Set Performance:**
This regularized version of the Decision Tree Classifier, with constraints like max_depth=5 and min_samples_split=27, shows balanced metrics on the training set.
It achieves an accuracy of 0.79, with precision, recall, and F1-score ranging from 0.61 to 0.87 across classes.
The ROC AUC score is 0.826, indicating good discrimination.

**Test Set Performance:**
On the test set, the model maintains a consistent accuracy of 0.79.
Metrics for class 0 are strong (precision=0.82, recall=0.88, F1-score=0.85), while class 1 metrics are slightly lower (precision=0.69, recall=0.60, F1-score=0.64).
The ROC AUC score is 0.825, remarkably close to the training score.

**Analysis:** The regularization appears effective in mitigating overfitting, as the model shows comparable performance on both sets, indicating better generalization than the unregularized dtcl.

"""

# Random Forest classifier

rfcl = RandomForestClassifier(random_state = 121)
rfcl.fit(X_train, y_train)

# rfcl model prediction on train data

y_train_predict = rfcl.predict(X_train)

# rfcl model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = rfcl.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of rfcl model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of rfcl model on training data.')

# rfcl model prediction on test data

y_test_predict = rfcl.predict(X_test)

# rfcl model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = rfcl.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of rfcl model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of rfcl model on test data.')

"""**Training Set Performance:**
The Random Forest Classifier (rfcl) exhibits exceptional performance on the training data, achieving near-perfect metrics with an accuracy of 1.00, precision, recall, and F1-score close to 1.00 for both classes (0 and 1).
The ROC AUC score is an impressive 0.99995, indicating excellent discrimination capability on the training set.

**Test Set Performance:**
On the test set, the performance drops significantly, with an accuracy of 0.74. Precision, recall, and F1-score for class 0 are around 0.79-0.85, but for class 1, they hover around 0.51-0.61, showing a struggle to predict the minority class effectively.
The ROC AUC score decreases to 0.76, reflecting good generalization to unseen data.

**Analysis:** The stark contrast between training and test performance suggests overfitting, where the model learns the training data too well, including noise, and fails to generalize to new data.
"""

# Tuning the parameters of the rfcl for a regularized random forest classifier model for better performance.

reg_rfcl_model = RandomForestClassifier(n_estimators = 120, criterion = 'entropy', max_depth = 7, min_samples_split = 19, min_samples_leaf = 9, max_features = 5, random_state = 121)
reg_rfcl_model.fit(X_train, y_train)

# reg_rfcl_model prediction on train data

y_train_predict = reg_rfcl_model.predict(X_train)

# reg_rfcl_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_rfcl_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_rfcl_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_rfcl_model on training data.')

# reg_rfcl_model prediction on test data

y_test_predict = reg_rfcl_model.predict(X_test)

# reg_rfcl_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_rfcl_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_rfcl_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_rfcl_model on test data.')

"""**Training Set Performance:**
The regularized Random Forest model achieves an accuracy of 0.80 on the training set, with precision, recall, and F1-score ranging from 0.61 to 0.90 across classes.
The ROC AUC score is 0.869, demonstrating strong predictive power.

**Test Set Performance (reg_rfcl_model):**
On the test set, the accuracy remains steady at 0.79.
Class 0 metrics are robust (precision=0.81, recall=0.89, F1-score=0.85), while class 1 metrics are moderate (precision=0.71, recall=0.56, F1-score=0.63).
Specific ROC AUC score for the test set is 0.81 which can be inferred to be close to training performance based on other metrics.

**Analysis:** The Random Forest model benefits from regularization, maintaining consistent performance across training and test sets, suggesting good generalization capabilities.

**Bagging Classifier**
"""

params = {
                'n_estimators' : [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]
}

bgcl = BaggingClassifier(estimator = reg_rfcl_model, random_state = 121)

grid_search = GridSearchCV(estimator = bgcl, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

bagging_model = grid_search.best_estimator_

# Best Hyperparameters: {'n_estimators': 25}

# bagging_model prediction on train data

y_train_predict = bagging_model.predict(X_train)

# bagging_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = bagging_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of bagging_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of bagging_model on training data.')

# bagging_model prediction on test data

y_test_predict = bagging_model.predict(X_test)

# bagging_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = bagging_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of bagging_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of bagging_model on test data.')

"""**Training Set Performance:**
On the training set, the Bagging Model achieves an accuracy of 0.79 across 2000 samples.
The classification report reveals strong metrics for class 0 (likely the majority class) with a precision of 0.82, recall of 0.89, and F1-score of 0.85 based on 1359 samples.
For class 1 (potentially the minority class), the metrics are slightly lower with a precision of 0.72, recall of 0.59, and F1-score of 0.64 across 641 samples.
The macro average F1-score is 0.75, and the weighted average F1-score is 0.79, indicating balanced performance across classes despite the disparity in support.
The ROC AUC score for the training data is 0.856, reflecting good discrimination capability between classes .

**Test Set Performance:**
On the test set, the Bagging Model maintains a comparable accuracy of 0.78 across 858 samples.
Metrics for class 0 remain strong, with a precision of 0.81, recall of 0.89, and F1-score of 0.85 based on 585 samples.
For class 1, with 273 samples, the model reports a precision of 0.70, recall of 0.57, and F1-score of 0.63, showing a slight dip in performance for the minority class.
The macro average F1-score is 0.74, and the weighted average F1-score is 0.78.
The ROC AUC score on the test set is 0.811, which, while slightly lower than the training score, still indicates solid discriminative power on unseen data .

**Analysis:** The Bagging model demonstrates solid performance on the test set, likely benefiting from the ensemble approach that reduces variance and overfitting compared to single models like the unregularized Decision Tree.

**AdaBoostClassifier**
"""

params = {
                'n_estimators' : [15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255, 270, 285, 300]
}

adaboost_clf = AdaBoostClassifier(random_state = 121)

grid_search = GridSearchCV(estimator = adaboost_clf, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

adaboost_model = grid_search.best_estimator_

# Best Hyperparameters: {'n_estimators': 195}

# adaboost_model prediction on train data

y_train_predict = adaboost_model.predict(X_train)

# adaboost_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = adaboost_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of adaboost_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of adaboost_model on training data.')

# adaboost_model prediction on test data

y_test_predict = adaboost_model.predict(X_test)

# adaboost_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = adaboost_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of adaboost_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of adaboost_model on test data.')

"""**Training Set Performance:**
1. The AdaBoost model achieves an accuracy of 0.77 on the training set.
2. Metrics for class 0 are robust (precision=0.80, recall=0.89, F1-score=0.84), while class 1 metrics lag (precision=0.69, recall=0.51, F1-score=0.59).
3. The ROC AUC score is 0.815, indicating strong discrimination ability.

**Test Set Performance:**
1. Performance on the test set mirrors the training set with an accuracy of 0.77.
2. Metrics for class 0 are robust (precision=0.80, recall=0.89, F1-score=0.84), while class 1 metrics lag (precision=0.68, recall=0.51, F1-score=0.59).
3. The ROC AUC score is 0.80, indicating strong discrimination ability.

**Analysis:** AdaBoost shows consistent performance across both sets, suggesting it handles generalization well, though it struggles with the minority class.

**GradientBoostingClassifier**
"""

params = {
                'n_estimators' : [10, 20, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360, 390]
}

gradient_boost_clf = GradientBoostingClassifier(random_state = 121)

grid_search = GridSearchCV(estimator = gradient_boost_clf, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

grad_boost_model = grid_search.best_estimator_

# Best Hyperparameters: {'n_estimators': 30}

# grad_boost_model prediction on train data

y_train_predict = grad_boost_model.predict(X_train)

# grad_boost_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = grad_boost_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of grad_boost_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of grad_boost_model on training data.')

# grad_boost_model prediction on test data

y_test_predict = grad_boost_model.predict(X_test)

# grad_boost_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = grad_boost_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of grad_boost_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of grad_boost_model on test data.')

"""**Training Set Performance:**
1. The grad_boost_model achieves an accuracy of 0.79 on the training set.
2. Metrics for class 0 are robust (precision=0.82, recall=0.90, F1-score=0.86), while class 1 metrics are moderate (precision=0.73, recall=0.57, F1-score=0.64).
3. The ROC AUC score is 0.84, indicating strong discrimination ability.

**Test Set Performance:**
1. Performance on the test set mirrors the training set with an accuracy of 0.79.
2. Metrics for class 0 are robust (precision=0.81, recall=0.90, F1-score=0.85), while class 1 metrics are reasonable (precision=0.72, recall=0.55, F1-score=0.62).
3. The ROC AUC score is 0.81, indicating strong discrimination ability.

**Analysis:** grad_boost_model shows consistent performance across both sets, suggesting it handles generalization well, though it struggles with the minority class.

**Artificial Neural Network**
"""

mlp_model = MLPClassifier(random_state = 121)
mlp_model.fit(X_train, y_train)

# mlp_model prediction on train data

y_train_predict = mlp_model.predict(X_train)

# mlp_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = mlp_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of mlp_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of mlp_model on training data.')

# mlp_model prediction on test data

y_test_predict = mlp_model.predict(X_test)

# mlp_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = mlp_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of mlp_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of mlp_model on test data.')

"""**Training Set Performance:**
1. The MLP model achieves an accuracy of 0.77 on the training set.
2. Metrics for class 0 are robust (precision=0.78, recall=0.92, F1-score=0.84), while class 1 metrics are poor (precision=0.72, recall=0.44, F1-score=0.55).
3. The ROC AUC score is 0.81, indicating reasonable discrimination ability.

**Test Set Performance:**
1. Performance on the test set mirrors the training set with an accuracy of 0.76.
2. Metrics for class 0 are robust (precision=0.77, recall=0.92, F1-score=0.84), while class 1 metrics are poor (precision=0.71, recall=0.41, F1-score=0.52).
3. The ROC AUC score is 0.79, indicating reasonable discrimination ability.

**Analysis:** The MLP model shows a drop in performance on the test set, particularly for class 1, indicating potential overfitting or challenges in handling imbalanced data and requires extensive tuning to generalize well .
"""

# Tuning the parameters of the mlp_model for better performance.
'''
params = {
    'hidden_layer_sizes': [(100), (150), (100, 50), (150, 75), (100,100,100)],
    'activation': ['logistic', 'relu'],
    'solver': ['sgd', 'adam'],
		'learning_rate_init': [0.1, 0.001, 0.0001],
    'tol': [0.1, 0.01, 0.001],
    'max_iter' : [1000, 4000, 7000, 10000],
		'early_stopping':[True, False]
}

grid_search = GridSearchCV(estimator = mlp_model, param_grid = params, cv = 5, scoring = 'accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

reg_mlp_model = grid_search.best_estimator_
'''
# Best Hyperparameters: {'activation': 'logistic', 'early_stopping': False, 'hidden_layer_sizes': 100, 'learning_rate_init': 0.001, 'max_iter': 1000, 'solver': 'adam', 'tol': 0.001}

#Regularized multilayer perceptron model

reg_mlp_model = MLPClassifier(activation = 'logistic', early_stopping = False, hidden_layer_sizes = 100, learning_rate_init = 0.001, max_iter = 1000, solver = 'adam', tol = 0.001, random_state = 121)

# Fit the model
reg_mlp_model.fit(X_train, y_train)

# reg_mlp_model prediction on train data

y_train_predict = reg_mlp_model.predict(X_train)

# reg_mlp_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_mlp_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_mlp_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title("ROC curve of reg_mlp_model on training data.")

# reg_mlp_model prediction on test data

y_test_predict = reg_mlp_model.predict(X_test)

# reg_mlp_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_mlp_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_mlp_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title("ROC curve of reg_mlp_model on test data.")

"""**Training Set Performance:**
1. The Regularized MLP model achieves an accuracy of 0.77 on the training set.
2. Metrics for class 0 are robust (precision=0.79, recall=0.91, F1-score=0.85), while class 1 metrics are poor (precision=0.72, recall=0.49, F1-score=0.58).
3. The ROC AUC score is 0.81, indicating reasonable discrimination ability.

**Test Set Performance:**
1. Performance on the test set mirrors the training set with an accuracy of 0.77.
2. Metrics for class 0 are robust (precision=0.79, recall=0.91, F1-score=0.84), while class 1 metrics are weak (precision=0.70, recall=0.47, F1-score=0.56).
3. The ROC AUC score is 0.79, indicating reasonable discrimination ability.

**Analysis:** Like MLP model, the Regularized MLP model shows a drop in performance on the test set, particularly for class 1, indicating potential overfitting or challenges in handling imbalanced data.

**KNeighborsClassifier Model**
"""

knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

# knn_model prediction on train data

y_train_predict = knn_model.predict(X_train)

# knn_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = knn_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of knn_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of knn_model on training data.')

# knn_model prediction on test data

y_test_predict = knn_model.predict(X_test)

# knn_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = knn_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of knn_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of knn_model on test data.')

"""**Training Set Performance:**
1. The KNeighborsClassifier model achieves an accuracy of 0.80 on the training set.
2. Metrics for class 0 are robust (precision=0.83, recall=0.90, F1-score=0.86), while class 1 metrics are weak (precision=0.73, recall=0.60, F1-score=0.66).
3. The ROC AUC score is 0.86, indicating strong discrimination ability.

**Test Set Performance:**
1. The KNeighborsClassifier model achieves an accuracy of 0.71 on the test set.
2. Metrics for class 0 are moderate (precision=0.78, recall=0.81, F1-score=0.79), while class 1 metrics are poor (precision=0.55, recall=0.50, F1-score=0.52).
3. The ROC AUC score is 0.72, reflecting poor generalization to unseen data.

**Analysis:** KNeighborsClassifier exhibits overfitting and shows a drop in performance on the test set, particularly for class 1, indicating potential challenges in handling imbalanced data.
"""

# Tuning the parameters of the knn_model for better performance.

grid_params = {
                'n_neighbors': [5, 10, 15, 25, 30, 35, 40, 45],
                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
                'leaf_size': [10, 15, 20, 25, 30]
}

grid_search = GridSearchCV(estimator = knn_model, param_grid = grid_params, cv=5, scoring='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

reg_knn_model = grid_search.best_estimator_

# Best Hyperparameters: {'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 25}

# reg_knn_model prediction on train data

y_train_predict = reg_knn_model.predict(X_train)

# reg_knn_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_knn_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_knn_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_knn_model on train data.')

# reg_knn_model prediction on test data

y_test_predict = reg_knn_model.predict(X_test)

# reg_knn_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_knn_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_knn_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_knn_model on test data.')

"""**Training Set Performance:**
1. The Regularized KNeighborsClassifier model achieves an accuracy of 0.75 on the training set.
2. Metrics for class 0 are robust (precision=0.77, recall=0.91, F1-score=0.83), while class 1 metrics are weak (precision=0.69, recall=0.41, F1-score=0.52).
3. The ROC AUC score is 0.78, indicating moderate discrimination ability.

**Test Set Performance:**
1. The Regularized KNeighborsClassifier model achieves an accuracy of 0.74 on the test set.
2. Metrics for class 0 are robust (precision=0.76, recall=0.91, F1-score=0.83), while class 1 metrics are weak (precision=0.66, recall=0.40, F1-score=0.50).
3. The ROC AUC score is 0.75, reflecting poor generalization to unseen data.

**Analysis:** Regularized KNeighborsClassifier shows a drop in performance on the test set, particularly for class 1, indicating potential challenges in handling imbalanced data.

**GaussianNB model**
"""

# GaussianNB model

gaussian_nb_model = GaussianNB()
gaussian_nb_model.fit(X_train, y_train)

# gaussian_nb_model prediction on train data

y_train_predict = gaussian_nb_model.predict(X_train)

# gaussian_nb_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = gaussian_nb_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of gaussian_nb_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of gaussian_nb_model on training data.')

# gaussian_nb_model prediction on test data

y_test_predict = gaussian_nb_model.predict(X_test)

# gaussian_nb_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = gaussian_nb_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of gaussian_nb_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of gaussian_nb_model on test data.')

"""**Training Set Performance:**
1. The GaussianNB model achieves an accuracy of 0.74 on the training set.
2. Metrics for class 0 are robust (precision=0.77, recall=0.88, F1-score=0.82), while class 1 metrics are poor (precision=0.64, recall=0.43, F1-score=0.53).
3. The ROC AUC score is 0.77, indicating moderate discrimination ability.

**Test Set Performance:**
1. The GaussianNB model achieves an accuracy of 0.74 on the test set.
2. Metrics for class 0 are robust (precision=0.77, recall=0.89, F1-score=0.83), while class 1 metrics are poor (precision=0.64, recall=0.44, F1-score=0.52).
3. The ROC AUC score is 0.77, reflecting moderate generalization to unseen data.

**Analysis:** GaussianNB model shows consistent performance across both sets, where it struggles with the minority class 1, indicating potential challenges in handling imbalanced data.

**Logistic Regression model**
"""

ss = StandardScaler()

X_train_scaled = ss.fit_transform(X_train)
X_train_scaled

X_test_scaled = ss.transform(X_test)
X_test_scaled

# Create LogisticRegression model

logistic_model = LogisticRegression(random_state = 121)
logistic_model.fit(X_train_scaled, y_train)

# Prediction of logistic_model on train data

y_train_predict = logistic_model.predict(X_train_scaled)
y_train_predict

# logistic_model performance on train data - confusion matrix

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = [0, 1], index = [0, 1])
cm_train

# logistic_model performance on train data - classification report

print(classification_report(y_train, y_train_predict))

# Probability of the predicted classes on train data

y_train_predict_prob = logistic_model.predict_proba(X_train_scaled)
y_train_predict_prob

# roc_auc_score of logistic_model on train data

y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_train = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of logistic_model on train data: {}'.format(roc_train))

# roc_curve of logistic_model on train data

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of logistic_model on train data.')

# Prediction of logistic_model on test data

y_test_predict = logistic_model.predict(X_test_scaled)
y_test_predict

# logistic_model performance on test data - confusion matrix

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = [0, 1], index = [0, 1])
cm_test

# logistic_model performance on test data - classification report

print(classification_report(y_test, y_test_predict))

# Probability of the predicted classes on test data

y_test_predict_prob = logistic_model.predict_proba(X_test_scaled)
y_test_predict_prob

# roc_auc_score of logistic_model on test data

y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_test = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of logistic_model on test data: {}'.format(roc_test))

# roc_curve of logistic_model on test data

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of logistic_model on test data.')

#Feature importance

print("Coefficient of independent features in X- \n")

for i, col in enumerate(X.columns):
  print(f"Coefficient of {col}: {logistic_model.coef_[0][i]}")

intercept = logistic_model.intercept_

print(f"\nintercept of logistic_model: {intercept}")

"""* In the logistic regression model, the coefficients of each feature indicate the strength and direction of the features impact on the probability of the outcome (in this case, whether a claim is made).
A positive coefficient suggests that an increase in the features value increases the likelihood of a claim, while a negative coefficient suggests the opposite.
The magnitude of the coefficient reflects the strength of the effect.

1. The coefficient for Age is -0.16380769339068385, implying that higher age are associated with a lower likelihood of a claim.

2. The coefficient for Agency_Code is -0.40063000755863504, implying that certain agencies are associated with a lower likelihood of claim.

3. The coefficient for Type is -0.5222444808492801, implying that certain tour insurance firms are associated with a lower likelihood of claim.

4. The Coefficient for Commision is -0.04623704381097521, implying that higher commision are associated with a lower likelihood of a claim.

5. The Coefficient for Channel is -0.12431294065291086, implying cartain distribution channel are associated with a lower likelihood of claim.

6. The Coefficient for Duration is -0.01898291548153528, implying that higher duration are associated with a lower likelihood of a claim.

7. The Coefficient for Sales: 0.4965596366814753 implying that higher sales values (more expensive policies) are associated with a higher likelihood of a claim.

8. The Coefficient for Product Name: 0.19651353816046765 implying that certain product are associated with a higher likelihood of claims.

9. The Coefficient for Destination: 0.11214557666335653 implying that certain destinations are associated with a lower likelihood of claim.

**Training Set Performance:**
1. The LogisticRegression model achieves an accuracy of 0.76 on the training set.
2. Metrics for class 0 are robust (precision=0.77, recall=0.91, F1-score=0.84), while class 1 metrics are weak (precision=0.69, recall=0.44, F1-score=0.54).
3. The ROC AUC score is 0.79, indicating moderate discrimination ability.

**Test Set Performance:**
1. Performance on the test set achieves an accuracy of 0.75.
2. Metrics for class 0 are robust (precision=0.77, recall=0.90, F1-score=0.83), while class 1 metrics are weak (precision=0.67, recall=0.43, F1-score=0.52).
3. The ROC AUC score is 0.79, indicating moderate generalization to unseen data.

**Analysis:** The LogisticRegression model shows consistent performance across both sets, where it struggles with the minority class 1, indicating potential challenges in handling imbalanced data.
"""

# Performing model tuning using GridSearchCV

params = {
          'penalty': ['l1', 'l2'],
          'solver': ['liblinear'],
          'max_iter': [175, 250, 325, 400, 475, 550, 625, 700, 775, 850, 925, 1000]
        }

grid_search = GridSearchCV(estimator = logistic_model, param_grid = params, cv = 3, n_jobs = 2)
grid_search.fit(X_train_scaled, y_train)

print('Best parameters: ', grid_search.best_params_,'\n')
print('Estimator: ', grid_search.best_estimator_)

reg_logistic_model = grid_search.best_estimator_

# Prediction of reg_logistic_model on train data

y_train_predict = reg_logistic_model.predict(X_train_scaled)
y_train_predict

# reg_logistic_model performance on train data - confusion matrix

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = [0, 1], index = [0, 1])
cm_train

# reg_logistic_model performance on train data - classification report

print(classification_report(y_train, y_train_predict))

# Probability of the predicted classes on train data

y_train_predict_prob = reg_logistic_model.predict_proba(X_train_scaled)
y_train_predict_prob

# roc_auc_score of reg_logistic_model on train data

y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_train = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_logistic_model on train data: {}'.format(roc_train))

# roc_curve of reg_logistic_model on train data

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_logistic_model on train data.')

# Prediction of reg_logistic_model on test data

y_test_predict = reg_logistic_model.predict(X_test_scaled)
y_test_predict

# reg_logistic_model performance on test data - confusion matrix

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = [0, 1], index = [0, 1])
cm_test

# reg_logistic_model performance on test data - classification report

print(classification_report(y_test, y_test_predict))

# Probability of the predicted classes on test data

y_test_predict_prob = reg_logistic_model.predict_proba(X_test_scaled)
y_test_predict_prob

# roc_auc_score of reg_logistic_model on test data

y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_test = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_logistic_model on test data: {}'.format(roc_test))

# roc_curve of reg_logistic_model on test data

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_logistic_model on test data.')

print("Coefficient of independent features in X- \n")

for i, col in enumerate(X.columns):
  print(f"Coefficient of {col}: {reg_logistic_model.coef_[0][i]}")

intercept = reg_logistic_model.intercept_

print(f"\nThe intercept of reg_logistic_model: {intercept}")

"""* In the regularized logistic regression model, the coefficients of each feature indicate the strength and direction of the feature's impact on the probability of the outcome (in this case, whether a claim is made).
A positive coefficient suggests that an increase in the feature's value increases the likelihood of a claim, while a negative coefficient suggests the opposite.
The magnitude of the coefficient reflects the strength of the effect.

1. The coefficient for Age is -0.16363966383618264, implying that higher age are associated with a lower likelihood of a claim.

2. The coefficient for Agency_Code is -0.5221563895117652, implying that certain agencies are associated with a lower likelihood of claim.

3. The coefficient for Type is -0.4002812398115419, implying that certain tour insurance firms are associated with a lower likelihood of claim.

4. The Coefficient of Commision is -0.04654867082162875, implying that higher Commision are associated with a lower likelihood of a claim.

5. The Coefficient of Channel is -0.12449496610186653, implying cartain distribution channel are associated with a lower likelihood of claim.

6. The Coefficient of Duration is -0.01901710850963336, implying that higher duration are associated with a lower likelihood of a claim.

7. The Coefficient of Sales: 0.49659988262857824 implying that higher sales values (more expensive policies) are associated with a higher likelihood of a claim.

8. The Coefficient of Product Name: 0.19654544066868512 implying that certain product are associated with a higher likelihood of claim.

9. The Coefficient of Destination: 0.11206394315308961 implying that certain destinations are associated with a higher likelihood of claim.

**Training Set Performance:**
1. The Regularized LogisticRegression model achieves an accuracy of 0.76 on the training set.
2. Metrics for class 0 are robust (precision=0.77, recall=0.91, F1-score=0.84), while class 1 metrics are poor (precision=0.69, recall=0.44, F1-score=0.54).
3. The ROC AUC score is 0.79, indicating reasonable discrimination ability.

**Test Set Performance:**
1. Performance on the test set achieves an accuracy of 0.75.
2. Metrics for class 0 are robust (precision=0.77, recall=0.90, F1-score=0.83), while class 1 metrics are poor (precision=0.67, recall=0.43, F1-score=0.52).
3. The ROC AUC score is 0.79, indicating reasonable generalization to unseen data.

**Analysis:** The Regularized LogisticRegression model shows consistent performance across both sets, where it struggles with the minority class 1, indicating potential challenges in handling imbalanced data.

**Linear Discriminant Analysis**
"""

# Create LinearDiscriminantAnalysis model

lda_model = LinearDiscriminantAnalysis()
lda_model.fit(X_train_scaled, y_train)

# Prediction of lda_model on train data

y_train_predict = lda_model.predict(X_train_scaled)
y_train_predict

# lda_model performance on train data - confusion matrix

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = [0, 1], index = [0, 1])
cm_train

# lda_model performance on train data - classification report

print(classification_report(y_train, y_train_predict))

# Probability of the predicted classes on train data

y_train_predict_prob = lda_model.predict_proba(X_train_scaled)
y_train_predict_prob

# roc_auc_score of lda_model on train data

y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_train = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of lda_model on train data: {}'.format(roc_train))

# roc_curve of lda_model on train data

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of lda_model on train data.')

# Prediction of lda_model on test data

y_test_predict = lda_model.predict(X_test_scaled)
y_test_predict

# lda_model performance on test data - confusion matrix

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = [0, 1], index = [0, 1])
cm_test

# lda_model performance on test data - classification report

print(classification_report(y_test, y_test_predict))

# Probability of the predicted classes on test data

y_test_predict_prob = lda_model.predict_proba(X_test_scaled)
y_test_predict_prob

# roc_auc_score of lda_model on test data

y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_test = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of lda_model on test data: {}'.format(roc_test))

# roc_curve of lda_model on test data

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of lda_model on test data.')

print("Coefficient of features in X:\n")

for i, col in enumerate(X.columns):
  print(f"The coefficient of {col}: {lda_model.coef_[0][i]}")

intercept = lda_model.intercept_

print(f"\nThe intercept of lda_model: {intercept}")

lda_model.explained_variance_ratio_

"""1. The coefficient for Age is -0.18426926894545173, implying that higher age are associated with a lower likelihood of a claim .
2. The coefficient for Agency_Code is -0.5820815275487989, implying that certain agencies are associated with a lower likelihood of claim.
3. The coefficient for Type is -0.5153261079955257, implying that certain tour insurance firms are associated with a lower likelihood of claim.
4. The Coefficient of Commision is -0.043432168819381806, implying that higher Commision are associated with a lower likelihood of a claim.
5. The Coefficient of Channel is -0.13918919029072152, implying cartain distribution channel are associated with a lower likelihood of claim.
6. The Coefficient of Duration is -0.015926954755487614, implying that higher duration are associated with a lower likelihood of a claim.
7. The Coefficient of Sales: 0.5410356335614136 implying that higher sales values (more expensive policies) are associated with a higher likelihood of a claim .
8. The Coefficient of Product Name: 0.27446062148523076 implying that certain product are associated with a higher likelihood of claim.
9. The Coefficient of Destination: 0.11214557666335653 implying that certain destinations are associated with a higher likelihood of claim.

**Training Set Performance:**
1. The LinearDiscriminantAnalysis model achieves an accuracy of 0.76 on the training set.
2. Metrics for class 0 are robust (precision=0.78, recall=0.91, F1-score=0.84), while class 1 metrics are poor (precision=0.69, recall=0.44, F1-score=0.54).
3. The ROC AUC score is 0.79, indicating reasonable discrimination ability.

**Test Set Performance:**
1. Performance on the test set achieves an accuracy of 0.75.
2. Metrics for class 0 are robust (precision=0.77, recall=0.90, F1-score=0.83), while class 1 metrics are poor (precision=0.68, recall=0.44, F1-score=0.53).
3. The ROC AUC score is 0.79, indicating reasonable generalization to unseen data.

**Analysis:** The LinearDiscriminantAnalysis model shows consistent performance across both sets, where it struggles with the minority class 1, indicating potential challenges in handling imbalanced data.
"""

# Performing model tuning using GridSearchCV

params = {
          'solver': ['lsqr', 'eigen'],
          'tol': [0.1, 0.001, 0.0001],
          'shrinkage': ['auto']
          }

grid_search = GridSearchCV(estimator = lda_model, param_grid = params, cv = 3, n_jobs = 2)
grid_search.fit(X_train, y_train)

print('Best parameters: ', grid_search.best_params_,'\n')
print('Estimator: ', grid_search.best_estimator_)

reg_lda_model = grid_search.best_estimator_

# Prediction of reg_lda_model on train data

y_train_predict = reg_lda_model.predict(X_train_scaled)
y_train_predict

# reg_lda_model performance on train data - confusion matrix

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = [0, 1], index = [0, 1])
cm_train

# reg_lda_model performance on train data - classification report

print(classification_report(y_train, y_train_predict))

# Probability of the predicted classes on train data

y_train_predict_prob = reg_lda_model.predict_proba(X_train_scaled)
y_train_predict_prob

# roc_auc_score of reg_lda_model on train data

y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_train = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_lda_model on train data: {}'.format(roc_train))

# roc_curve of reg_lda_model on train data

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel("fpr")
plt.ylabel("tpr")
plt.title('ROC curve of reg_lda_model on train data.')

# Prediction of reg_lda_model on test data

y_test_predict = reg_lda_model.predict(X_test_scaled)
y_test_predict

# reg_lda_model performance on test data - confusion matrix

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = [0, 1], index = [0, 1])
cm_test

# reg_lda_model performance on test data - classification report

print(classification_report(y_test, y_test_predict))

# Probability of the predicted classes on test data

y_test_predict_prob = reg_lda_model.predict_proba(X_test_scaled)
y_test_predict_prob

# roc_auc_score of reg_lda_model on test data

y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_test = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_lda_model on test data: {}'.format(roc_test))

# roc_curve of reg_lda_model on test data

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')

plt.title('ROC curve of new_lda_model on test data.')

print("Coefficient of features in X:")

for i, col in enumerate(X.columns):
  print(f"The coefficient of {col}: {reg_lda_model.coef_[0][i]}")

intercept = reg_lda_model.intercept_

print(f"\nThe intercept of reg_lda_model: {intercept}")

lda_model.explained_variance_ratio_

"""1. The coefficient for Age is -0.016843942419583757, implying that higher age are associated with a lower likelihood of a claim.
2. The coefficient for Agency_Code is -0.6251360494835558, implying that certain agencies are associated with a lower likelihood of claim.
3. The coefficient for Type is -0.044296299663872, implying that certain tour insurance firms are associated with a lower likelihood of claim.
4. The Coefficient of Commision is -0.0023517323044113247, implying that higher Commision are associated with a slightly lower likelihood of a claim.
5. The Coefficient of Channel is -1.091742752117355, implying cartain distribution channel are associated with a lower likelihood of claim.
6. The Coefficient of Duration is 5.76041378615999e-05, implying that higher duration are associated with a slightly higher likelihood of a claim.
7. The Coefficient of Sales: 0.006717213317820436 implying that higher sales values (more expensive policies) are associated with a slightly higher likelihood of a claim .
8. The Coefficient of Product Name: 0.2209259094669822 implying that certain product are associated with a higher likelihood of claim.
9. The Coefficient of Destination: 0.11240546513273031 implying that certain destinations are associated with a higher likelihood of claim.

**Training Set Performance:**
1. The Regularized LinearDiscriminantAnalysis model achieves an accuracy of 0.59 on the training set.
2. Metrics for class 0 are moderate (precision=0.86, recall=0.48, F1-score=0.62), for class 1 metrics are also poor (precision=0.43, recall=0.83, F1-score=0.57).
3. The ROC AUC score is 0.76, indicating moderate discrimination ability.

**Test Set Performance:**
1. Performance on the test set achieves an accuracy of 0.61.
2. Metrics for class 0 are moderate (precision=0.89, recall=0.49, F1-score=0.63), for class 1 metrics are also poor (precision=0.44, recall=0.87, F1-score=0.59).
3. The ROC AUC score is 0.76, indicating weak discrimination ability

**Analysis:** The Regularized LinearDiscriminantAnalysis model shows consistent performance across both sets, where it struggles with the majority class 0, indicating potential challenges in handling imbalanced data.

**Optimal Model for classification:**

---



Based on the provided test data metrics, the **reg_rfcl_model (regularized Random Forest classifier)** appears to be the best model for classification.
* This is because it achieves a high accuracy of 0.79 and a good balance between precision and recall across both classes (0 and 1).
* Its F1-score, a measure that balances precision and recall, is also relatively high at 0.85 and 0.63 for classes 0 and 1 respectively.  
* Furthermore, it has a strong ROC AUC score of 0.81 on the test data, indicating good overall discrimination between the classes.
* While other models show high scores on training data, reg_rfcl_model shows strong generalization to unseen test data, which is crucial for a good classifier.
* Other models show better scores on the training set but underperform on the test set, suggesting overfitting.
"""

