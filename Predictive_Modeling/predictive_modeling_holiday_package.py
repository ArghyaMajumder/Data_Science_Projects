# -*- coding: utf-8 -*-
"""Predictive_Modeling_Holiday_Package.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iI69gil7gtrmjUcJKboG_CMTgM1XyuTJ
"""

from google.colab import files, drive

upload = files.upload()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.linear_model import LogisticRegression

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from sklearn.neighbors import KNeighborsClassifier

from sklearn.naive_bayes import GaussianNB

from sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree

from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier

from sklearn.neural_network import MLPClassifier

from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

holiday_df = pd.read_csv('Holiday_Package.csv')
holiday_df

# Checking the datatype of all the columns

holiday_df.info()

# Since Unnamed: 0 column is of no use the column is being dropped

holiday_df.drop(columns = ['Unnamed: 0'], inplace = True)
holiday_df

# Check for missing value in holiday_df

holiday_df.isnull().sum()

# Check for duplicate rows in holiday_df

print('Number of duplicate rows in holiday_df: {}\n'.format(holiday_df[holiday_df.duplicated].shape[0]))
holiday_df[holiday_df.duplicated()]

print('Number of employees opted and not opted the holiday package - \n{}\n'.format(holiday_df['Holliday_Package'].value_counts()))
print('Percentage of employees opted and not opted the holiday package - \n{}\n'.format(holiday_df['Holliday_Package'].value_counts(normalize = True)))

"""**Since 54% of the employees have not opted for the holiday package, we can say that the dataset is biased towards not opting the holiday package**"""

plt.figure(figsize = (10, 4))
sns.histplot(x = 'Salary', data = holiday_df, kde = True)
plt.title("Distribution of the employee salary ")

plt.figure(figsize = (10, 4))
sns.histplot(x = 'Salary', data = holiday_df, kde = True, hue = 'Holliday_Package')
plt.title("Distribution of the salary of employees who opted and not opted ")

"""1. Some employees with salary below 150000 have opted while some have not opted the holiday package.
2. Employees with salary 150000 and above have not opted the holiday package.

**Salary above 150000 is a separator between employess who have not opted the holiday package and those who have opted the package**
"""

plt.figure(figsize = (10, 3))
sns.scatterplot(x = 'Salary', y = 'Holliday_Package', data = holiday_df)
plt.title("Salary of employees opted and not opted the Holliday_Package ")

print('Number of employees having different age - \n{}\n'.format(holiday_df['age'].value_counts()))

plt.figure(figsize = (15, 4))
sns.countplot(x = 'age', data = holiday_df)
plt.title("Number of employees having different age ")

plt.figure(figsize = (20, 4))
sns.countplot(x = 'age', data = holiday_df, hue = 'Holliday_Package')
plt.title("Number of employees having different age and opting holiday package")

"""1. Majority of employess of age 20, 21, 26, 27, 32 - 39, 41 - 45 and 48 are those who opted the holiday package.
2. Majority of employess of age 22, 23, 24, 25, 46, 47, and 49 onwards are those who have not opted the holiday package.
"""

plt.figure(figsize = (10, 3))
sns.scatterplot(x = 'age', y = 'Holliday_Package', data = holiday_df)
plt.title("Age of the employees opted and not opted the holiday package ")

print('Number of employees having different years of formal education - \n{}\n\n'.format(holiday_df['educ'].value_counts()))
print('Percentage of employees having different years of formal education - \n{}\n'.format(holiday_df['educ'].value_counts(normalize= True)))

plt.figure(figsize = (15, 4))
sns.countplot(x = 'educ', data = holiday_df)
plt.title("Number of employees having different years of formal education")

plt.figure(figsize = (20, 4))
sns.countplot(x = 'educ', data = holiday_df, hue = 'Holliday_Package')
plt.title("Number of employees having different years of formal education and opting holiday package")

"""1. Most of the employees having 1, 2, 3, 4, 5, 6, 7, 13, 15, 16 and 18 years of formal education have opted the holiday package.

2. Most of the employees having 8, 9, 10, 11, 12, 14, 19 and 21 years of formal education have not opted the holiday package
"""

plt.figure(figsize = (10, 3))
sns.scatterplot(x = 'educ', y = 'Holliday_Package', data = holiday_df)
plt.title("employees having different years of formal education and opting holiday package")

# Number of employees having children younger than 7 years

print('Number of employees having children younger than 7 years - \n{}\n\n'.format(holiday_df['no_young_children'].value_counts()))
print('Percentage of employees having children younger than 7 years - \n{}\n'.format(holiday_df['no_young_children'].value_counts(normalize= True)))

plt.figure(figsize = (8, 3))
sns.countplot(x = 'no_young_children', data = holiday_df)
plt.title("Number of employees having children younger than 7 years")

plt.figure(figsize = (10, 3))
sns.countplot(x = 'no_young_children', data = holiday_df, hue = 'Holliday_Package')
plt.title("Number of employees having children younger than 7 years and opting holiday package")

"""Most of the employees having no children younger than 7 years have opted for the holiday package"""

plt.figure(figsize = (10, 3))
sns.scatterplot(x = 'no_young_children', y = 'Holliday_Package', data = holiday_df)
plt.title("Number of employees having children younger than 7 years and opting holiday package")

# Number of employees having children older than 7 years

print('Number of employees having children older than 7 years - \n{}\n\n'.format(holiday_df['no_older_children'].value_counts()))
print('Percentage of employees having children older than 7 years - \n{}\n'.format(holiday_df['no_older_children'].value_counts(normalize= True)))

plt.figure(figsize = (8, 3))
sns.countplot(x = 'no_older_children', data = holiday_df)
plt.title("Number of employees having children older than 7 years")

plt.figure(figsize = (10, 3))
sns.countplot(x = 'no_older_children', data = holiday_df, hue = 'Holliday_Package')
plt.title("Number of employees having children older than 7 years and opting holiday package")

"""1. Majority of the employees having 2, 3 and 6 children above 7 years have opted the holiday package.
2. Majority of the employees having at least 1 children above 7 years have not opted the holiday package.
"""

plt.figure(figsize = (10, 3))
sns.scatterplot(x = 'no_older_children', y = 'Holliday_Package', data = holiday_df)
plt.title("Employees having children older than 7 years and opting holiday package")

print('Number of foreign employees - \n{}\n'.format(holiday_df['foreign'].value_counts()))
print('Percentage of foreign employees - \n{}\n'.format(holiday_df['foreign'].value_counts(normalize = True)))

plt.figure(figsize = (6, 3))
sns.countplot(x = 'foreign', data = holiday_df)
plt.title("Number of foreign employees")

plt.figure(figsize = (7, 3))
sns.countplot(x = 'foreign', data = holiday_df, hue = 'Holliday_Package')
plt.title("Number of foreign employees and opting holiday package")

"""1. Most of the foreign employees have opted the holliday package.
2. Around 1/3rd of the domestic employees have opted the holiday package.
"""

plt.figure(figsize = (10, 3))
sns.scatterplot(x = 'foreign', y = 'Holliday_Package', data = holiday_df)
plt.title("Foreign employees and opting holiday package")

# pair plot exhibiting the correlation between features in holiday_df

sns.pairplot(holiday_df, hue = 'Holliday_Package')

holiday_df.describe()

plt.figure(figsize = (10, 6))
sns.boxplot(data = holiday_df, orient = 'h')

plt.figure(figsize = (6, 4))
sns.heatmap(holiday_df[['Salary', 'age', 'educ', 'no_young_children', 'no_older_children']].corr(), annot = True)
plt.title("Heatmap displaying the correlation between the features in holiday_df")

# Converting the object types to categorical types

for feature in holiday_df.columns:
  if holiday_df[feature].dtype == 'object':
    print('feature: ', feature)
    print(pd.Categorical(holiday_df[feature].unique()))
    print(pd.Categorical(holiday_df[feature].unique()).codes)
    holiday_df[feature] = pd.Categorical(holiday_df[feature]).codes
    print('\n\n')

holiday_df

# Separating the dependent and independent variables

X = holiday_df.drop(columns = ['Holliday_Package'])
y = holiday_df['Holliday_Package']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 121)

X_train

y_train

X_test

y_test

ss = StandardScaler()
X_train_scaled = ss.fit_transform(X_train)
X_train_scaled

X_test_scaled = ss.transform(X_test)
X_test_scaled

"""**Logistic Regression**"""

# Create LogisticRegression model

logistic_model = LogisticRegression(random_state = 121)
logistic_model.fit(X_train_scaled, y_train)

# Prediction of logistic_model on train data

y_train_predict = logistic_model.predict(X_train_scaled)
y_train_predict

# logistic_model performance on train data - confusion matrix

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = [0, 1], index = [0, 1])
cm_train

# logistic_model performance on train data - classification report

print(classification_report(y_train, y_train_predict))

# Probability of the predicted classes on train data

y_train_predict_prob = logistic_model.predict_proba(X_train_scaled)
y_train_predict_prob

# roc_auc_score of logistic_model on train data

y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_train = roc_auc_score(y_train, y_train_predict_prob_pos)
print('roc_auc_score of logistic_model on train data: {}'.format(roc_train))

# roc_curve of logistic_model on train data

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of logistic_model on train data.')

# Prediction of logistic_model on test data

y_test_predict = logistic_model.predict(X_test_scaled)
y_test_predict

# logistic_model performance on test data - confusion matrix

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = [0, 1], index = [0, 1])
cm_test

# logistic_model performance on test data - classification report

print(classification_report(y_test, y_test_predict))

# Probability of the predicted classes on test data

y_test_predict_prob = logistic_model.predict_proba(X_test_scaled)
y_test_predict_prob

# roc_auc_score of logistic_model on test data

y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_test = roc_auc_score(y_test, y_test_predict_prob_pos)
print('roc_auc_score of logistic_model on test data: {}'.format(roc_test))

# roc_curve of logistic_model on test data

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of logistic_model on test data.')

print("Coefficient of independent features in X- \n")

for i, col in enumerate(X.columns):
  print(f"Coefficient of {col}: {logistic_model.coef_[0][i]}")

intercept = logistic_model.intercept_

print(f"intercept of logistic_model: {intercept}")

"""* The coefficients of the independent features in the logistic regression model indicate the direction and magnitude of the impact each feature has on the log-odds of the outcome (whether or not an employee opts for a holiday package).

* A negative coefficient suggests that as the feature increases, the likelihood of opting for a holiday package decreases, while a positive coefficient indicates the opposite.

* These coefficients suggest that **higher salary, older age, and having young or older children** are associated with a lower likelihood of opting for a holiday package.

* Conversely, **higher education and being a foreign employee** are associated with a higher likelihood of opting for a holiday package.

* The magnitude of the coefficient reflects the strength of the association. The **number of young children** has a relatively large negative coefficient (-0.8167), indicating a strong negative impact on the likelihood of opting for a holiday package.

**Training Set Performance:**
1. The LogisticRegression model achieves an accuracy of 0.68 on the training set.
2. Class 0 metrics are moderate (precision=0.67, recall=0.74, F1-score=0.70), while Class 1 metrics are weak (precision=0.69, recall=0.62, F1-score=0.65),
3. The ROC AUC score is 0.74, demonstrating moderate predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.69 .
2. Class 0 metrics are moderate (precision=0.75, recall=0.73, F1-score=0.74), while class 1 metrics are weak (precision=0.61, recall=0.65, F1-score=0.63).
3. The ROC AUC score is 0.72, demonstrating moderate predictive power.

**Analysis:** The LogisticRegression model maintains consistent performance across training and test sets, suggesting good generalization capabilities. There is a drop in performance on the test set, particularly for class 1, indicating challenges in handling imbalanced data.
"""

# Performing model tuning using GridSearchCV

'''
params = {'penalty': ['l1', 'l2', 'None'],
          'solver': ['liblinear', 'newton-cholesky'],
          'max_iter': [100, 150, 200, 250, 300, 400, 450, 500, 1000]}
'''

params = {
          'penalty': ['l1', 'l2'],
          'solver': ['liblinear'],
          'max_iter': [100, 150, 200, 250, 300, 400, 450, 500, 1000]
        }

grid_search = GridSearchCV(estimator = logistic_model, param_grid = params, cv = 3, n_jobs = 2)
grid_search.fit(X_train_scaled, y_train)

print('Best parameters: ', grid_search.best_params_,'\n')
print('Estimator: ', grid_search.best_estimator_)

reg_logistic_model = grid_search.best_estimator_

# Prediction of reg_logistic_model on train data

y_train_predict = reg_logistic_model.predict(X_train_scaled)
y_train_predict

# reg_logistic_model performance on train data - confusion matrix

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = [0, 1], index = [0, 1])
cm_train

# reg_logistic_model performance on train data - classification report

print(classification_report(y_train, y_train_predict))

# Probability of the predicted classes on train data

y_train_predict_prob = reg_logistic_model.predict_proba(X_train_scaled)
y_train_predict_prob

# roc_auc_score of reg_logistic_model on train data

y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_train = roc_auc_score(y_train, y_train_predict_prob_pos)
print('roc_auc_score of reg_logistic_model on train data: {}'.format(roc_train))

# roc_curve of reg_logistic_model on train data

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_logistic_model on train data.')

# Prediction of reg_logistic_model on test data

y_test_predict = reg_logistic_model.predict(X_test_scaled)
y_test_predict

# reg_logistic_model performance on test data - confusion matrix

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = [0, 1], index = [0, 1])
cm_test

# reg_logistic_model performance on test data - classification report

print(classification_report(y_test, y_test_predict))

# Probability of the predicted classes on test data

y_test_predict_prob = reg_logistic_model.predict_proba(X_test_scaled)
y_test_predict_prob

# roc_auc_score of reg_logistic_model on test data

y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_test = roc_auc_score(y_test, y_test_predict_prob_pos)
print('roc_auc_score of reg_logistic_model on test data: {}'.format(roc_test))

# roc_curve of reg_logistic_model on test data

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_logistic_model on test data.')

print("Coefficient of independent features in X- \n")

for i, col in enumerate(X.columns):
  print(f"Coefficient of {col}: {reg_logistic_model.coef_[0][i]}")

intercept = reg_logistic_model.intercept_

print(f"The intercept of reg_logistic_model: {intercept}")

"""* The coefficients of the independent features in the regularized logistic regression model indicate the direction and magnitude of the impact each feature has on the log-odds of the outcome (whether or not an employee opts for a holiday package).

* A negative coefficient suggests that as the feature increases, the likelihood of opting for a holiday package decreases, while a positive coefficient indicates the opposite.

* These values suggest that **higher salary, older age, and having young or older children are associated with a lower likelihood of opting for a holiday package**.

* Conversely, **higher education and being a foreign employee are associated with a higher likelihood of opting for a holiday package**.

* The magnitude of the coefficient reflects the strength of the association.
The **number of young children** has a relatively large negative coefficient (-0.80), indicating a strong negative impact on the likelihood of opting for a holiday package.

**Training Set Performance:**
1. The regularized LogisticRegression model achieves an accuracy of 0.68 on the training set.
2. Class 0 metrics are moderate (precision=0.68, recall=0.74, F1-score=0.71), while Class 1 metrics are weak (precision=0.69, recall=0.63, F1-score=0.66),
3. The ROC AUC score is 0.74, demonstrating moderate predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.69.
2. Class 0 metrics are moderate (precision=0.75, recall=0.73, F1-score=0.74), while class 1 metrics are weak (precision=0.61, recall=0.65, F1-score=0.63).
3. The ROC AUC score is 0.72, demonstrating moderate predictive power.

**Analysis:** The regularized LogisticRegression model maintains consistent performance across training and test sets, suggesting moderate generalization capabilities.

**Performing Linear Discriminant Analysis -**
"""

# Create LinearDiscriminantAnalysis model

lda_model = LinearDiscriminantAnalysis()
lda_model.fit(X_train_scaled, y_train)

# Prediction of lda_model on train data

y_train_predict = lda_model.predict(X_train_scaled)
y_train_predict

# lda_model performance on train data - confusion matrix

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = [0, 1], index = [0, 1])
cm_train

# lda_model performance on train data - classification report

print(classification_report(y_train, y_train_predict))

# Probability of the predicted classes on train data

y_train_predict_prob = lda_model.predict_proba(X_train_scaled)
y_train_predict_prob

# roc_auc_score of lda_model on train data

y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_train = roc_auc_score(y_train, y_train_predict_prob_pos)
print('roc_auc_score of lda_model on train data: {}'.format(roc_train))

# roc_curve of lda_model on train data

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of lda_model on train data.')

# Prediction of lda_model on test data

y_test_predict = lda_model.predict(X_test_scaled)
y_test_predict

# lda_model performance on test data - confusion matrix

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = [0, 1], index = [0, 1])
cm_test

# lda_model performance on test data - classification report

print(classification_report(y_test, y_test_predict))

# Probability of the predicted classes on test data

y_test_predict_prob = lda_model.predict_proba(X_test_scaled)
y_test_predict_prob

# roc_auc_score of lda_model on test data

y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_test = roc_auc_score(y_test, y_test_predict_prob_pos)
print('roc_auc_score of lda_model on test data: {}'.format(roc_test))

# roc_curve of lda_model on test data

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of lda_model on test data.')

print("Coefficient of features in X:")

for i, col in enumerate(X.columns):
  print(f"The coefficient of {col}: {lda_model.coef_[0][i]}")

intercept = lda_model.intercept_

print(f"The intercept of lda_model: {intercept}")

"""* The coefficients of the independent features in the Linear Discriminant Analysis (LDA) model indicate the direction and magnitude of each feature's impact on the model's decision boundary.

* A coefficient reveals how much the LDA model shifts its classification threshold due to a one-unit change in the corresponding feature.

* These coefficients suggest that **higher salary, older age, and having more young or older children are associated with a decreased likelihood of opting for a holiday package**.

* Conversely, **higher education and being a foreign employee are associated with an increased likelihood of opting for a holiday package**.

* The absolute value of the coefficient indicates the feature's importance,  **foreign has a coefficient of 0.6018, suggesting a notable positive influence, while no_older_children has a very small coefficient of -0.0214, indicating a minimal impact** .
"""

lda_model.explained_variance_ratio_

"""**Training Set Performance:**
1. The LinearDiscriminantAnalysis model achieves an accuracy of 0.68 on the training set.
2. Class 0 metrics are moderate (precision=0.67, recall=0.74, F1-score=0.70), while Class 1 metrics are weak (precision=0.69, recall=0.61, F1-score=0.65),
3. The ROC AUC score is 0.74, demonstrating moderate predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.69.
2. Class 0 metrics are moderate (precision=0.75, recall=0.73, F1-score=0.74), while class 1 metrics are weak (precision=0.61, recall=0.64, F1-score=0.62).
3. The ROC AUC score is 0.72, demonstrating moderate predictive power.

**Analysis:** The LinearDiscriminantAnalysis model maintains consistent performance across training and test sets, suggesting good generalization capabilities.

**Regularized Linear Discriminant Analysis**
"""

# Performing model tuning using GridSearchCV

params = {
          'solver': ['lsqr', 'eigen'],
          'tol': [0.1, 0.001, 0.0001],
          'shrinkage': ['auto']
          }

grid_search = GridSearchCV(estimator = lda_model, param_grid = params, cv = 3, n_jobs = 2)
grid_search.fit(X_train, y_train)

print('Best parameters: ', grid_search.best_params_,'\n')
print('Estimator: ', grid_search.best_estimator_)

reg_lda_model = grid_search.best_estimator_

# Prediction of reg_lda_model on train data

y_train_predict = reg_lda_model.predict(X_train_scaled)
y_train_predict

# reg_lda_model performance on train data - confusion matrix

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = [0, 1], index = [0, 1])
cm_train

# reg_lda_model performance on train data - classification report

print(classification_report(y_train, y_train_predict))

# Probability of the predicted classes on train data

y_train_predict_prob = reg_lda_model.predict_proba(X_train_scaled)
y_train_predict_prob

# roc_auc_score of reg_lda_model on train data

y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_train = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_lda_model on train data: {}'.format(roc_train))

# roc_curve of reg_lda_model on train data

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_lda_model on train data.')

# Prediction of reg_lda_model on test data

y_test_predict = reg_lda_model.predict(X_test_scaled)
y_test_predict

# reg_lda_model performance on test data - confusion matrix

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = [0, 1], index = [0, 1])
cm_test

# reg_lda_model performance on test data - classification report

print(classification_report(y_test, y_test_predict))

# Probability of the predicted classes on test data

y_test_predict_prob = reg_lda_model.predict_proba(X_test_scaled)
y_test_predict_prob

# roc_auc_score of reg_lda_model on test data

y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_test = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_lda_model on test data: {}'.format(roc_test))

# roc_curve of reg_lda_model on test data

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of new_lda_model on test data.')

print("Coefficient of features in X:")

for i, col in enumerate(X.columns):
  print(f"The coefficient of {col}: {reg_lda_model.coef_[0][i]}")

intercept = reg_lda_model.intercept_

print(f"The intercept of reg_lda_model: {intercept}")

"""* The coefficients of the independent features in the Linear Discriminant Analysis (LDA) model indicate the direction and magnitude of each feature's impact on the model's decision boundary.

* A coefficient reveals how much the LDA model shifts its classification threshold due to a one-unit change in the corresponding feature.

* These coefficients suggest that **higher salary, older age, and having more young children are associated with a decreased likelihood of opting for a holiday package**.

* Conversely, **higher education, having more older children and being a foreign employee are associated with an increased likelihood of opting for a holiday package**.


* The absolute value of the coefficient indicates the feature's importance. **foreign has a coefficient of 1.32, suggesting a notable positive influence, while older age has a very small coefficient of -0.035, indicating a minimal impact**.
"""

lda_model.explained_variance_ratio_

"""**Training Set Performance:**
1. The regularized LinearDiscriminantAnalysis model achieves an accuracy of 0.58 on the training set.
2. Class 0 metrics are poor (precision=0.79, recall=0.25, F1-score=0.39), while Class 1 metrics are weak (precision=0.54, recall=0.93, F1-score=0.68),
3. The ROC AUC score is 0.70, demonstrating moderate predictive power.

**Test Set Performance:**
1. On the test set, the accuracy remains steady at 0.50 .
2. Class 0 metrics are poor (precision=0.78, recall=0.22, F1-score=0.35), while class 1 metrics are weak (precision=0.44, recall=0.90, F1-score=0.59).
3. The ROC AUC score is 0.70, demonstrating moderate predictive power.

**Analysis:** The regularized LinearDiscriminantAnalysis model maintains consistent performance across training and test sets, suggesting poor generalization capabilities. There is a drop in performance on the test set, particularly for class 0.

**K-Nearest Neighbors**
"""

X = holiday_df[['Salary', 'age', 'educ', 'no_young_children', 'no_older_children', 'foreign']]
y = holiday_df['Holliday_Package']

X

y

# KNeighborsClassifier model

knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

# knn_model prediction on train data

y_train_predict = knn_model.predict(X_train)

# knn_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = knn_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of knn_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of knn_model on training data.')

# knn_model prediction on test data

y_test_predict = knn_model.predict(X_test)

# knn_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = knn_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of knn_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of knn_model on test data.')

"""**Training Set Performance:**
1. The KNeighborsClassifier model achieves an accuracy of 0.70 on the training set.
2. Class 0 metrics are moderate (precision=0.72, recall=0.68, F1-score=0.70), while Class 1 metrics are moderate (precision=0.68, recall=0.72, F1-score=0.70),
3. The ROC AUC score is 0.76, demonstrating moderate predictive power.

**Test Set Performance:**
1. The KNeighborsClassifier model achieves an accuracy of 0.54 on the test set.
2. Class 0 metrics are weak (precision=0.63, recall=0.57, F1-score=0.60), while class 1 metrics are poor (precision=0.43, recall=0.50, F1-score=0.46).
3. The ROC AUC score is 0.53, demonstrating poor predictive power.

**Analysis:** The KNeighborsClassifier model exibits inconsistent performance across training and test sets, suggesting poor generalization capabilities. There is a drop in performance on the test set, particularly for class 1.
"""

# Tuning the parameters of the knn_model for better performance.

grid_params = {
                'n_neighbors': [25, 30, 35, 40, 45],
                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
                'leaf_size': [10, 15, 20, 25, 30]
}

grid_search = GridSearchCV(estimator = knn_model, param_grid = grid_params, cv=5, scoring='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

reg_knn_model = grid_search.best_estimator_

# reg_knn_model prediction on train data

y_train_predict = reg_knn_model.predict(X_train)

# reg_knn_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_knn_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_knn_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_knn_model on train data.')

# reg_knn_model prediction on test data

y_test_predict = reg_knn_model.predict(X_test)

# reg_knn_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_knn_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_knn_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_knn_model on test data.')

"""**Training Set Performance:**
1. The regularized KNeighborsClassifier model achieves an accuracy of 0.60 on the training set.
2. Metrics are poor for both Class 0 (precision=0.62, recall=0.56, F1-score=0.59) and Class 1 (precision=0.58, recall=0.64, F1-score=0.61),
3. The ROC AUC score is 0.64, demonstrating weak predictive power.

**Test Set Performance:**
1. The regularized KNeighborsClassifier model achieves an accuracy of 0.55 on the test set.
2. Metrics are poor for both Class 0 (precision=0.67, recall=0.50, F1-score=0.57) and class 1 (precision=0.46, recall=0.64, F1-score=0.53).
3. The ROC AUC score is 0.57, demonstrating poor predictive power.

**Analysis:** The regularized KNeighborsClassifier model exibits consistent performance across training and test sets, suggesting poor generalization capabilities. There is a drop in performance on the test set, particularly for class 1.

**GaussianNB model**
"""

# GaussianNB model

gaussian_nb_model = GaussianNB()
gaussian_nb_model.fit(X_train, y_train)

# gaussian_nb_model prediction on train data

y_train_predict = gaussian_nb_model.predict(X_train)

# gaussian_nb_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = gaussian_nb_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of gaussian_nb_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of gaussian_nb_model on training data.')

# gaussian_nb_model prediction on test data

y_test_predict = gaussian_nb_model.predict(X_test)

# gaussian_nb_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = gaussian_nb_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of gaussian_nb_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of gaussian_nb_model on test data.')

"""**Training Set Performance:**
1. The GaussianNB model achieves an accuracy of 0.65 on the training set.
2. Metrics are weak for both Class 0 (precision=0.73, recall=0.50, F1-score=0.60) and Class 1 (precision=0.60, recall=0.80, F1-score=0.69),
3. The ROC AUC score is 0.71, demonstrating moderate predictive power.

**Test Set Performance:**
1. The GaussianNB model achieves an accuracy of 0.58 on the test set.
2. Class 0 metrics are poor (precision=0.76, recall=0.43, F1-score=0.55), while class 1 metrics are weak (precision=0.48, recall=0.79, F1-score=0.60).
3. The ROC AUC score is 0.68, demonstrating weak predictive power.

**Analysis:** The GaussianNB model exibits consistent performance across training and test sets, suggesting weak generalization capabilities. GaussianNB model shows a drop in performance on the test set, particularly for class 1.

**Decision Tree**
"""

# Decision Tree Classifier

dtcl = DecisionTreeClassifier(criterion = 'gini', random_state = 121)
dtcl.fit(X_train, y_train)

from google.colab import drive

drive.mount('/content/drive')
file = open('/content/drive/My Drive/Colab Notebooks/holiday_package_tree.dot', 'w')
train_char_label = ['1', '0']
dot_data = export_graphviz(dtcl, out_file = file, feature_names = list(X_train), class_names = list(train_char_label))
file.close()

# dtcl prediction on train data

y_train_predict = dtcl.predict(X_train)

# dtcl performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = dtcl.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of dtcl on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of dtcl on training data.')

# dtcl prediction on test data

y_test_predict = dtcl.predict(X_test)

# dtcl performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = dtcl.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of dtcl on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of dtcl on test data.')

"""**Training Set Performance:**
1. The DecisionTreeClassifier model achieves an accuracy of 1.00 on the training set.
2. Metrics are outstanding for both Class 0 (precision=1.00, recall=1.00, F1-score=1.00) and Class 1 (precision=1.00, recall=1.00, F1-score=1.00),
3. The ROC AUC score is 1.00, demonstrating outstanding predictive power.

**Test Set Performance:**
1. The DecisionTreeClassifier model achieves an accuracy of 0.60 on the test set.
2. Class 0 metrics are weak (precision=0.69, recall=0.59, F1-score=0.64), while class 1 metrics are poor (precision=0.50, recall=0.60, F1-score=0.54).
3. The ROC AUC score is 0.60, demonstrating weak predictive power.

**Analysis:** The stark contrast between training and test performance suggests overfitting, where the model learns the training data too well, including noise, and fails to generalize to new data.
"""

# Tuning the parameters of the dtcl for better performance.

params = {
                'criterion': ['gini', 'entropy', 'log_loss'],
                'max_depth': [4, 5, 6, 7, 8, 9, 10, 11],
                'min_samples_split': [2, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25],
                'min_samples_leaf':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
                'max_features':[3, 4, 5, 'log2', 'sqrt']
}

grid_search = GridSearchCV(estimator = dtcl, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

reg_dtcl_model = grid_search.best_estimator_

# reg_dtcl_model = DecisionTreeClassifier(criterion = 'gini', max_depth = 10, max_features = 'log2', min_samples_leaf = 9, min_samples_split = 25, random_state = 121)
# Best Hyperparameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 9, 'min_samples_split': 25}

# reg_dtcl_model prediction on train data

y_train_predict = reg_dtcl_model.predict(X_train)

# reg_dtcl_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_dtcl_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_dtcl_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_dtcl_model on training data.')

# reg_dtcl_model prediction on test data

y_test_predict = reg_dtcl_model.predict(X_test)

# reg_dtcl_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_dtcl_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_dtcl_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_dtcl_model on test data.')

"""**Training Set Performance:**
1. The regularized DecisionTreeClassifier model achieves an accuracy of 0.71 on the training set.
2. Metrics for class 0 are moderate (precision=0.69, recall=0.79, F1-score=0.74), while class 1 metrics are weak (precision=0.74, recall=0.63, F1-score=0.68).
3. The ROC AUC score is 0.77, indicating moderate discrimination ability.

**Test Set Performance:**
1. The regularized DecisionTreeClassifier model achieves an accuracy of 0.68 on the test set.
2. Metrics for class 0 are moderate (precision=0.74, recall=0.73, F1-score=0.73), while class 1 metrics are weak (precision=0.60, recall=0.61, F1-score=0.61).
3. The ROC AUC score is 0.73, reflecting moderate generalization to unseen data.

**Analysis:** The regularized DecisionTreeClassifier model maintains consistent performance across training and test sets, suggesting moderate generalization capabilities. There is a drop in performance on the test set, particularly for class 1, indicating challenges in handling imbalanced data.

**Random Forest**
"""

# Random Forest classifier

rfcl = RandomForestClassifier(random_state = 121)
rfcl.fit(X_train, y_train)

# rfcl model prediction on train data

y_train_predict = rfcl.predict(X_train)

# rfcl model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = rfcl.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of rfcl model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of rfcl model on training data.')

# rfcl model prediction on test data

y_test_predict = rfcl.predict(X_test)

# rfcl model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = rfcl.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of rfcl model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of rfcl model on test data.')

"""**Training Set Performance:**
1. The RandomForest model achieves an accuracy of 1.00 on the training set.
2. Metrics are outstanding for both Class 0 (precision=1.00, recall=1.00, F1-score=1.00) and Class 1 (precision=1.00, recall=1.00, F1-score=1.00),
3. The ROC AUC score is 1.00, demonstrating outstanding predictive power.

**Test Set Performance:**
1. On the test set, the RandomForest model achieves an weak accuracy of 0.68 .
2. Class 0 metrics are moderate (precision=0.76, recall=0.69, F1-score=0.72), while class 1 metrics are weak (precision=0.59, recall=0.67, F1-score=0.62).
3. The ROC AUC score is 0.73, demonstrating moderate predictive power.

**Analysis:** The stark contrast between training and test performance suggests overfitting, where the model learns the training data too well, including noise, and fails to generalize to new data.
"""

# Tuning the parameters of the rfcl for a regularized random forest classifier model for better performance.

reg_rfcl_model = RandomForestClassifier(criterion = 'entropy', max_depth = 10, max_features = 5, min_samples_leaf = 9, min_samples_split = 19, n_estimators = 100, random_state = 121)
reg_rfcl_model.fit(X_train, y_train)

# reg_rfcl_model prediction on train data

y_train_predict = reg_rfcl_model.predict(X_train)

# reg_rfcl_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_rfcl_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_rfcl_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_rfcl_model on training data.')

# reg_rfcl_model prediction on test data

y_test_predict = reg_rfcl_model.predict(X_test)

# reg_rfcl_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_rfcl_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_rfcl_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_rfcl_model on test data.')

"""**Training Set Performance:**
1. The regularized RandomForest model achieves an accuracy of 0.79 on the training set.
2. Metrics are moderate for both class 0 (precision=0.80, recall=0.79, F1-score=0.79) and class 1 (precision=0.78, recall=0.79, F1-score=0.79).
3. The ROC AUC score is 0.87, indicating strong discrimination ability.

**Test Set Performance:**
1. The regularized RandomForest model achieves an accuracy of 0.69 on the test set.
2. Metrics for class 0 are moderate (precision=0.77, recall=0.69, F1-score=0.73), while class 1 metrics are weak (precision=0.60, recall=0.70, F1-score=0.64).
3. The ROC AUC score is 0.73, reflecting moderate generalization to unseen data.

**Analysis:** The regularized RandomForest model maintains consistent performance across training and test sets, suggesting moderate generalization capabilities. There is a drop in performance on the test set, particularly for class 1, indicating challenges in handling imbalanced data.

**Bagging Classifier**
"""

params = {
                'n_estimators' : [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]
}

bgcl = BaggingClassifier(estimator = reg_rfcl_model, random_state = 121)

grid_search = GridSearchCV(estimator = bgcl, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

bagging_model = grid_search.best_estimator_

# Best Hyperparameters: {'n_estimators': 45}

# bagging_model prediction on train data

y_train_predict = bagging_model.predict(X_train)

# bagging_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = bagging_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of bagging_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of bagging_model on training data.')

# bagging_model prediction on test data

y_test_predict = bagging_model.predict(X_test)

# bagging_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = bagging_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of bagging_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of bagging_model on test data.')

"""**Training Set Performance:**
1. The Bagging model achieves an accuracy of 0.75 on the training set.
2. Metrics are moderate for both class 0 (precision=0.75, recall=0.77, F1-score=0.76) and class 1 (precision=0.75, recall=0.73, F1-score=0.74).
3. The ROC AUC score is 0.83, indicating strong discrimination ability.

**Test Set Performance:**
1. The Bagging model achieves an accuracy of 0.69 on the test set.
2. Metrics for class 0 are moderate (precision=0.77, recall=0.68, F1-score=0.72), while class 1 metrics are weak (precision=0.59, recall=0.70, F1-score=0.64).
3. The ROC AUC score is 0.73, reflecting moderate generalization to unseen data.

**Analysis:** The regularized RandomForest model maintains consistent performance across training and test sets, suggesting moderate generalization capabilities. There is a drop in performance on the test set, particularly for class 1, indicating challenges in handling imbalanced data.

**AdaBoostClassifier**
"""

params = {
                'n_estimators' : [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 70, 80, 90, 100, 150]
}

adaboost_clf = AdaBoostClassifier(random_state = 121)

grid_search = GridSearchCV(estimator = adaboost_clf, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

adaboost_model = grid_search.best_estimator_

# adaboost_model prediction on train data

y_train_predict = adaboost_model.predict(X_train)

# adaboost_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = adaboost_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of adaboost_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of adaboost_model on training data.')

# adaboost_model prediction on test data

y_test_predict = adaboost_model.predict(X_test)

# adaboost_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = adaboost_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of adaboost_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of adaboost_model on test data.')

"""**Training Set Performance:**
1. The AdaBoost model achieves an accuracy of 0.70 on the training set.
2. Metrics are moderate for both class 0 (precision=0.72, recall=0.71,F1-score=0.71) and class 1 (precision=0.69, recall=0.70, F1-score=0.70).
3. The ROC AUC score is 0.78, indicating moderate discrimination ability.

**Test Set Performance:**
1. The AdaBoost model achieves an accuracy of 0.68 on the test set.
2. Metrics for class 0 are moderate (precision=0.77, recall=0.66, F1-score=0.71), while class 1 metrics are weak (precision=0.58, recall=0.70, F1-score=0.64).
3. The ROC AUC score is 0.73, reflecting moderate generalization to unseen data.

**Analysis:** The regularized RandomForest model maintains consistent performance across training and test sets, suggesting moderate generalization capabilities. There is a drop in performance on the test set, particularly for class 1, indicating challenges in handling imbalanced data.

**GradientBoostingClassifier**
"""

params = {
                'n_estimators' : [10, 20, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360, 390]
}

gradient_boost_clf = GradientBoostingClassifier(random_state = 121)

grid_search = GridSearchCV(estimator = gradient_boost_clf, param_grid = params, cv = 7, scoring ='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

grad_boost_model = grid_search.best_estimator_

# Best Hyperparameters: {'n_estimators': 10}

# grad_boost_model prediction on train data

y_train_predict = grad_boost_model.predict(X_train)

# grad_boost_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = grad_boost_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)
print('roc_auc_score of grad_boost_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of grad_boost_model on training data.')

# grad_boost_model prediction on test data

y_test_predict = grad_boost_model.predict(X_test)

# grad_boost_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = grad_boost_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of grad_boost_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of grad_boost_model on test data.')

"""**Training Set Performance:**
1. The GradientBoostingClassifier model achieves an accuracy of 0.72 on the training set.
2. Metrics are moderate for both class 0 (precision=0.72, recall=0.75, F1-score=0.74) and class 1 (precision=0.72, recall=0.69, F1-score=0.71).
3. The ROC AUC score is 0.80, indicating strong discrimination ability.

**Test Set Performance:**
1. The GradientBoostingClassifier model achieves an accuracy of 0.69 on the test set.
2. Metrics for class 0 are moderate (precision=0.78, recall=0.68, F1-score=0.73), while class 1 metrics are weak (precision=0.60, recall=0.70, F1-score=0.65).
3. The ROC AUC score is 0.75, reflecting moderate generalization to unseen data.

**Analysis:** The GradientBoostingClassifier model maintains consistent performance across training and test sets, suggesting moderate generalization capabilities. There is a drop in performance on the test set, particularly for class 1, indicating challenges in handling imbalanced data.

**Artificial Neural Network**
"""

mlp_model = MLPClassifier(random_state = 121)
mlp_model.fit(X_train, y_train)

# mlp_model prediction on train data

y_train_predict = mlp_model.predict(X_train)

# mlp_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = mlp_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of mlp_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of mlp_model on training data.')

# mlp_model prediction on test data

y_test_predict = mlp_model.predict(X_test)

# mlp_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = mlp_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of mlp_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of mlp_model on test data.')

"""**Training Set Performance:**
1. The MLPClassifier model achieves an accuracy of 0.52 on the training set.
2. Metrics for class 0 are weak (precision=0.52, recall=1.00, F1-score=0.68), while class 1 metrics are poor (precision=1.00, recall=0.00, F1-score=0.01).
3. The ROC AUC score is 0.62, indicating weak discrimination ability.

**Test Set Performance:**
1. The MLPClassifier model achieves an accuracy of 0.60 on the test set.
2. Metrics for class 0 are moderate (precision=0.60, recall=1.00, F1-score=0.75), while class 1 metrics are poor (precision=0.00, recall=0.00, F1-score=0.00).
3. The ROC AUC score is 0.56, reflecting poor generalization to unseen data.

**Analysis:** The MLPClassifier model exhibits poor performance across training and test sets, it shows highly bias towards Class 0 and a poor classifier of Class 1 in both train and test set.
"""

# Tuning the parameters of the mlp_model for better performance.

params = {
    'hidden_layer_sizes': [(100), (150), (100, 50), (150, 75), (100,100,100)],
    'activation': ['logistic', 'relu'],
    'solver': ['sgd', 'adam'],
		'learning_rate_init': [0.1, 0.001, 0.0001],
    'tol': [0.1, 0.01, 0.001],
    'max_iter' : [1000, 4000, 7000, 10000],
		'early_stopping':[True, False]
}

grid_search = GridSearchCV(estimator = mlp_model, param_grid = params, cv = 5, scoring = 'accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters
print("Best Hyperparameters:", grid_search.best_params_)

reg_mlp_model = grid_search.best_estimator_

# Best Hyperparameters: {'activation': 'relu', 'early_stopping': False, 'hidden_layer_sizes': 100, 'learning_rate_init': 0.0001, 'max_iter': 1000, 'solver': 'adam', 'tol': 0.01}

# reg_mlp_model prediction on train data

y_train_predict = reg_mlp_model.predict(X_train)

# reg_mlp_model performance on train data

cm_train = pd.DataFrame(confusion_matrix(y_train, y_train_predict), columns = ['0', '1'], index = ['0', '1'])
cm_train

print(classification_report(y_train, y_train_predict))

y_train_predict_prob = reg_mlp_model.predict_proba(X_train)
y_train_predict_prob_pos = y_train_predict_prob[:, 1]
roc_score = roc_auc_score(y_train, y_train_predict_prob_pos)

print('roc_auc_score of reg_mlp_model on train data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_train, y_train_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_mlp_model on training data.')

# reg_mlp_model prediction on train data

y_test_predict = reg_mlp_model.predict(X_test)

# reg_mlp_model performance on test data

cm_test = pd.DataFrame(confusion_matrix(y_test, y_test_predict), columns = ['0', '1'], index = ['0', '1'])
cm_test

print(classification_report(y_test, y_test_predict))

y_test_predict_prob = reg_mlp_model.predict_proba(X_test)
y_test_predict_prob_pos = y_test_predict_prob[:, 1]
roc_score = roc_auc_score(y_test, y_test_predict_prob_pos)

print('roc_auc_score of reg_mlp_model on test data: {}'.format(roc_score))

fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_prob_pos)

plt.plot([0, 1], [0, 1], linestyle = '--')
plt.plot(fpr, tpr, marker = '*')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('ROC curve of reg_mlp_model on test data.')

"""**Training Set Performance:**
1. The regularized MLPClassifier model achieves an accuracy of 0.52 on the training set.
2. Metrics for class 0 are weak (precision=0.52, recall=0.88, F1-score=0.65), while class 1 metrics are poor (precision=0.51, recall=0.13, F1-score=0.20).
3. The ROC AUC score is 0.61, indicating weak discrimination ability.

**Test Set Performance:**
1. The regularized MLPClassifier model achieves an accuracy of 0.57 on the test set.
2. Metrics for class 0 are moderate (precision=0.60, recall=0.88, F1-score=0.71), while class 1 metrics are poor (precision=0.39, recall=0.11, F1-score=0.18).
3. The ROC AUC score is 0.52, reflecting poor generalization to unseen data.

**Analysis:** The regularized MLPClassifier model maintains consistent performance across training and test sets, it shows highly bias towards Class 0 and a poor classifier of Class 1 in both train and test set.

Best model:

---
Based on the provided analysis, the **GradientBoostingClassifier** appears to have the optimum performance on the dataset .

Reasons:

1.  **Balanced Performance:** It maintains consistent performance across both training and test datasets, suggesting good generalization.

2.   **Relatively High ROC AUC Score:** The GradientBoostingClassifier achieves a ROC AUC score of 0.80 on the training data and 0.75 on the test data, demonstrating a strong discrimination ability .

3.   **Accounts for Imbalanced Data:** While there's a performance drop for class 1 on the test set (as seen in other models), GradientBoostingClassifier handles the imbalanced data better than the Artificial Neural Network (MLPClassifier) .

While the RandomForest model also shows good performance, the GradientBoostingClassifier is chosen for its balance between performance and generalization.
"""

